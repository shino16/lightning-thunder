l_a_.new_empty((1, 7168), dtype = torch.bfloat16)
output_parallel.is_cpu
q_nope.new_empty((1, 32, 512))
torch.bmm(transpose, l_self_w_vc, out = transpose_1)
torch.cuda.get_device_capability(0)
torch.cuda.streams.Stream(stream_id = 35, device_index = 0, device_type = 1)
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 14, constant_args_idx = 15, grid = [(48, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'A': l_a_, 'B': l_b_, 'C': l_c_, 'As': l_as_, 'Bs': l_bs_})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 15, constant_args_idx = 23, grid = [(1, 5, 1)], tma_descriptor_metadata = {}, kwargs = {'kv_buffer_ptr': l_forward_batch_token_to_kv_pool_kv_buffer_0_, 'cache_k_nope_ptr': l_k_, 'cache_k_rope_ptr': l_k_rope_, 'loc_ptr': l_forward_batch_out_cache_loc})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = 20, constant_args_idx = 29, grid = [(32, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'y_ptr': input_2d, 'y_q_ptr': x_q, 'y_s_ptr': x_s})
torch.ops.sgl_kernel.dsv3_router_gemm(output, l_hidden_states_, l_self_modules_gate_parameters_weight_)
torch.ops.sglang.inplace_all_reduce(output_parallel, group_name = 'tp:0')
torch.ops.sglang.inplace_fused_experts(l_hidden_states_, l_self_modules_experts_parameters_w13_weight_, l_self_modules_experts_parameters_w2_weight_, l_stack0_topk_weights, l_stack0_topk_ids, None, None, 'silu', False, True, False, False, False, False, l_self_modules_experts_parameters_w13_weight_scale_inv_, l_self_modules_experts_parameters_w2_weight_scale_inv_, None, None, None, None, [128, 128], 2.5, None, None)
torch.ops.sglang.reg_all_gather_into_tensor(output_tensor, logits, group_name = 'tp:0')
