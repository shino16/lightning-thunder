VAR.is_cpu
VAR.new_empty((INT, INT), dtype = torch.bfloat16)
VAR.new_empty((INT, INT, INT))
torch._C._autograd._get_data_attr(VAR)
torch.bmm(VAR, l_self_w_vc, out = transpose_1)
torch.cuda.current_stream()
torch.cuda.get_device_capability()
torch.cuda.get_device_capability(INT)
torch.cuda.streams.Stream(stream_id = INT, device_index = INT, device_type = INT)
torch.empty((INT, INT), dtype = torch.uint32, device = device(type='cuda', index=INT))
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'A':VAR, 'B':VAR, 'C':VAR, 'As':VAR, 'Bs':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'B':VAR, 'Ret':VAR, 'Partials':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'GatherIndx':VAR, 'ScatterIndx':VAR, 'GateScal':VAR, 'ExptScal':VAR, 'ExptIndx':VAR, 'PartialOffs':VAR, 'TokensStart':VAR, 'Hist':VAR, 'MDTileStarts':VAR, 'MDTileInfo':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'Indx':VAR, 'ExpertHist':VAR, 'FinalExpertOffs':VAR, 'PartialHist':VAR, 'MDStarts':VAR, 'MDTileInfo':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'Y':VAR, 'Out':VAR, 'X':VAR, 'XPtr':VAR, 'W':VAR, 'B':VAR, 'GatherIndx':VAR, 'ExptHist':VAR, 'ExptOffs':VAR, 'ExptOffsSum':VAR, 'ExptData':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'kv_buffer_ptr':VAR, 'cache_k_nope_ptr':VAR, 'cache_k_rope_ptr':VAR, 'loc_ptr':VAR})
torch.ops.higher_order.triton_kernel_wrapper_mutation(kernel_idx = INT, constant_args_idx = INT, grid = [(INT, INT, INT)], tma_descriptor_metadata = {}, kwargs = {'y_ptr':VAR, 'y_q_ptr':VAR, 'y_s_ptr':VAR})
torch.ops.sgl_kernel.dsv3_router_gemm(VAR, l_hidden_states_,VAR)
torch.ops.sgl_kernel.rmsnorm.default(VAR, l_hidden_states_,VAR, 1e-INT, True)
torch.ops.sglang.flashinfer_allreduce_residual_rmsnorm(input_tensor = l_hidden_states_, residual = l_residual_, weight = l_self_layer_communicator_input_layernorm_parameters_weight_, eps = 1e-INT)
torch.ops.sglang.flashinfer_allreduce_residual_rmsnorm(input_tensor = l_stack0_, residual = l_residual_, weight = l_self_layer_communicator_post_attention_layernorm_parameters_weight_, eps = 1e-INT)
torch.ops.sglang.inplace_all_reduce(VAR, group_name = 'tp:INT')
torch.ops.sglang.inplace_fused_experts(VAR, l_self_modules_experts_parameters_w13_weight_,VAR, l_stack0_topk_weights,VAR, None, None, 'silu', False, True, False, False, False, False,VAR, l_self_modules_experts_parameters_w2_weight_scale_inv_, None, None, None, None, [INT, INT], INT.INT, None, None)
torch.ops.sglang.reg_all_gather_into_tensor(VAR, logits, group_name = 'tp:INT')
torch.transpose(VAR, INT, INT)
transpose[slice(None, INT, None)]
