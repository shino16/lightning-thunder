/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 04:06:50 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-21 04:06:50 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-21 04:06:51 TP0] sglang is using nccl==2.28.6
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-21 04:06:52 TP0] Init torch distributed ends. mem usage=1.46 GB
[2025-10-21 04:06:54 TP0] Load weight begin. avail mem=273.81 GB
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 486, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 660, in __init__
    self.model = Qwen3MoeModel(
                 ^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 638, in __init__
    super().__init__(
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 474, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 476, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 498, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 101, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 486, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 660, in __init__
    self.model = Qwen3MoeModel(
                 ^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 638, in __init__
    super().__init__(
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 474, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 476, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 498, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 101, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Process Process-3:
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 486, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 660, in __init__
    self.model = Qwen3MoeModel(
                 ^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 638, in __init__
    super().__init__(
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 474, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 476, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 498, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 101, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 486, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 660, in __init__
    self.model = Qwen3MoeModel(
                 ^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 638, in __init__
    super().__init__(
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 474, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen2_moe.py", line 476, in <lambda>
    lambda idx, prefix: decoder_layer_type(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 498, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/qwen3_moe.py", line 101, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
[rank1]:[W1021 04:06:55.865984578 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1021 04:06:55.941642033 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W1021 04:06:55.137648927 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W1021 04:06:55.157851071 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

================================================================================
ThunderFX debug info saved to: /opt/pytorch/lightning-thunder/gm/unsloth/Qwen3-30B-A3B-GGUF
Files:
  - log.txt (22,277 bytes)
  - rank0_0.py (10,194 bytes)
  - rank0_1.py (3,619 bytes)
  - rank0_10.py (4,946 bytes)
  - rank0_11.py (3,332 bytes)
  - rank0_12.py (21,891 bytes)
  - rank0_13.py (4,550 bytes)
  - rank0_14.py (8,205 bytes)
  - rank0_2.py (1,368 bytes)
  - rank0_3.py (3,225 bytes)
  - rank0_4.py (3,225 bytes)
  - rank0_5.py (15,751 bytes)
  - rank0_6.py (1,657 bytes)
  - rank0_7.py (4,921 bytes)
  - rank0_8.py (1,512 bytes)
  - rank0_9.py (21,890 bytes)
  - rank1_0.py (10,210 bytes)
  - rank1_1.py (3,619 bytes)
  - rank1_10.py (4,946 bytes)
  - rank1_11.py (3,332 bytes)
  - rank1_12.py (21,891 bytes)
  - rank1_13.py (4,550 bytes)
  - rank1_14.py (8,205 bytes)
  - rank1_2.py (1,368 bytes)
  - rank1_3.py (3,225 bytes)
  - rank1_4.py (3,225 bytes)
  - rank1_5.py (15,751 bytes)
  - rank1_6.py (1,657 bytes)
  - rank1_7.py (4,921 bytes)
  - rank1_8.py (1,512 bytes)
  - rank1_9.py (21,890 bytes)
  - rank2_0.py (10,212 bytes)
  - rank2_1.py (3,619 bytes)
  - rank2_10.py (4,946 bytes)
  - rank2_11.py (3,332 bytes)
  - rank2_12.py (21,891 bytes)
  - rank2_13.py (4,550 bytes)
  - rank2_14.py (8,205 bytes)
  - rank2_2.py (1,368 bytes)
  - rank2_3.py (3,225 bytes)
  - rank2_4.py (3,225 bytes)
  - rank2_5.py (15,751 bytes)
  - rank2_6.py (1,657 bytes)
  - rank2_7.py (4,921 bytes)
  - rank2_8.py (1,512 bytes)
  - rank2_9.py (21,890 bytes)
  - rank3_0.py (10,216 bytes)
  - rank3_1.py (3,619 bytes)
  - rank3_10.py (4,946 bytes)
  - rank3_11.py (3,332 bytes)
  - rank3_12.py (21,891 bytes)
  - rank3_13.py (4,550 bytes)
  - rank3_14.py (8,205 bytes)
  - rank3_2.py (1,368 bytes)
  - rank3_3.py (3,225 bytes)
  - rank3_4.py (3,225 bytes)
  - rank3_5.py (15,751 bytes)
  - rank3_6.py (1,657 bytes)
  - rank3_7.py (4,921 bytes)
  - rank3_8.py (1,512 bytes)
  - rank3_9.py (21,890 bytes)
================================================================================

