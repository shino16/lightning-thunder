/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:64: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-21 03:53:01 TP1] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 03:53:01 TP2] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 03:53:01 TP0] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 03:53:01 TP0] Init torch distributed begin.
[2025-10-21 03:53:01 TP3] Multimodal is disabled for llama4. To enable it, set --enable-multimodal.
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-21 03:53:02 TP0] sglang is using nccl==2.28.6
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-21 03:53:03 TP0] Init torch distributed ends. mem usage=1.46 GB
[2025-10-21 03:53:04 TP1] No vision weights found in checkpoint. Model will run in text-only mode. Multimodal capabilities (vision understanding) will be unavailable. Please not that this warning might be inaccurate if the weights haven't been fully downloaded
[2025-10-21 03:53:04 TP0] Load weight begin. avail mem=273.81 GB
[2025-10-21 03:53:04 TP0] No vision weights found in checkpoint. Model will run in text-only mode. Multimodal capabilities (vision understanding) will be unavailable. Please not that this warning might be inaccurate if the weights haven't been fully downloaded
[2025-10-21 03:53:04 TP3] No vision weights found in checkpoint. Model will run in text-only mode. Multimodal capabilities (vision understanding) will be unavailable. Please not that this warning might be inaccurate if the weights haven't been fully downloaded
[2025-10-21 03:53:04 TP2] No vision weights found in checkpoint. Model will run in text-only mode. Multimodal capabilities (vision understanding) will be unavailable. Please not that this warning might be inaccurate if the weights haven't been fully downloaded
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 613, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/mllama4.py", line 463, in __init__
    self.language_model = Llama4ForCausalLM(
                          ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 544, in __init__
    super().__init__(config, quant_config, prefix)
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama.py", line 421, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 558, in _init_model
    return Llama4Model(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 488, in __init__
    self.layers = make_layers(
                  ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 490, in <lambda>
    lambda idx, prefix: Llama4DecoderLayer(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 388, in __init__
    self.feed_forward = Llama4MoE(
                        ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 113, in __init__
    self.experts = FusedMoE(
                   ^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 613, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/mllama4.py", line 463, in __init__
    self.language_model = Llama4ForCausalLM(
                          ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 544, in __init__
    super().__init__(config, quant_config, prefix)
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama.py", line 421, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 558, in _init_model
    return Llama4Model(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 488, in __init__
    self.layers = make_layers(
                  ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 490, in <lambda>
    lambda idx, prefix: Llama4DecoderLayer(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 388, in __init__
    self.feed_forward = Llama4MoE(
                        ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 113, in __init__
    self.experts = FusedMoE(
                   ^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 613, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/mllama4.py", line 463, in __init__
    self.language_model = Llama4ForCausalLM(
                          ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 544, in __init__
    super().__init__(config, quant_config, prefix)
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama.py", line 421, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 558, in _init_model
    return Llama4Model(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 488, in __init__
    self.layers = make_layers(
                  ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 490, in <lambda>
    lambda idx, prefix: Llama4DecoderLayer(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 388, in __init__
    self.feed_forward = Llama4MoE(
                        ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 113, in __init__
    self.experts = FusedMoE(
                   ^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 524, in latency_test
    model_runner, tokenizer = load_model(server_args, port_args, tp_rank)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/bench_one_batch.py", line 156, in load_model
    model_runner = ModelRunner(
                   ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 261, in __init__
    self.initialize(min_per_gpu_memory)
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 308, in initialize
    self.load_model()
  File "/opt/sglang/sglang-src/python/sglang/srt/model_executor/model_runner.py", line 757, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 613, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/model_loader/loader.py", line 228, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/mllama4.py", line 463, in __init__
    self.language_model = Llama4ForCausalLM(
                          ^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 544, in __init__
    super().__init__(config, quant_config, prefix)
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama.py", line 421, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 558, in _init_model
    return Llama4Model(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 488, in __init__
    self.layers = make_layers(
                  ^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 508, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/utils.py", line 510, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 490, in <lambda>
    lambda idx, prefix: Llama4DecoderLayer(
                        ^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 388, in __init__
    self.feed_forward = Llama4MoE(
                        ^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/models/llama4.py", line 113, in __init__
    self.experts = FusedMoE(
                   ^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 216, in __init__
    self.quant_method: FusedMoEMethodBase = UnquantizedFusedMoEMethod(
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/quantization/unquant.py", line 147, in __init__
    from sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe import (
  File "/opt/sglang/sglang-src/python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py", line 9, in <module>
    from triton_kernels.matmul_ogs import (
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs.py", line 15, in <module>
    from .matmul_ogs_details._matmul_ogs import _compute_writeback_idx
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/matmul_ogs_details/_matmul_ogs.py", line 8, in <module>
    from triton_kernels.numerics_details.flexpoint import float_to_flex, load_scale
  File "/usr/local/lib/python3.12/dist-packages/triton_kernels/numerics_details/flexpoint.py", line 55, in <module>
    @tl.constexpr_function
     ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'triton.language' has no attribute 'constexpr_function'
[rank1]:[W1021 03:53:05.884874125 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W1021 03:53:05.893029575 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W1021 03:53:05.955596593 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W1021 03:53:05.985908755 ProcessGroupNCCL.cpp:1536] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

================================================================================
ThunderFX debug info saved to: /opt/pytorch/lightning-thunder/gm/meta-llama/Llama-4-Maverick-17B-128E-Instruct
Files:
  - log.txt (25,264 bytes)
================================================================================

