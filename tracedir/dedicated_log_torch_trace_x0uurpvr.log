V0914 01:44:29.205000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "52bd5960146f23872f87754e9290e530"}
	{
	"name": "dynamo",
	"ts": 1757839469204898.2,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:29.206000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "2a7048bf1bb106b0b87dff59f9f55584"}
	{
	"name": "entire_frame_compile",
	"ts": 1757839469206122.5,
	"args": {
	"fn_name": "_compile.compile_inner",
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:29.207000 107386 torch/_logging/structured.py:28] {"str": ["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", 0]}
V0914 01:44:29.207000 107386 torch/_logging/structured.py:28] {"str": ["/opt/pytorch/lightning-thunder/thunder/benchmarks/benchmark_litgpt.py", 1]}
V0914 01:44:29.208000 107386 torch/_logging/structured.py:28] {"str": ["/usr/local/lib/python3.12/dist-packages/jsonargparse/_cli.py", 2]}
V0914 01:44:29.208000 107386 torch/_logging/structured.py:28] {"str": ["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", 3]}
V0914 01:44:29.208000 107386 torch/_logging/structured.py:28] {"str": ["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", 4]}
V0914 01:44:29.209000 107386 torch/_logging/structured.py:28] {"str": ["/usr/local/lib/python3.12/dist-packages/litgpt/model.py", 5]}
V0914 01:44:29.209000 107386 torch/_dynamo/convert_frame.py:231] {"dynamo_start": {"stack": [{"line": 1154, "name": "<module>", "filename": 1, "loc": "CLI(benchmark_main)"}, {"line": 96, "name": "CLI", "filename": 2, "loc": "return _run_component(components, init)"}, {"line": 204, "name": "_run_component", "filename": 2, "loc": "return component(**cfg)"}, {"line": 1049, "name": "benchmark_main", "filename": 1, "loc": "benchmark.train()"}, {"line": 906, "name": "train", "filename": 1, "loc": "logits = self.model(input_ids)"}, {"line": 413, "name": "__call__", "filename": 3, "loc": "return super().__call__(*args, **kwargs)"}, {"line": 1775, "name": "_wrapped_call_impl", "filename": 4, "loc": "return self._call_impl(*args, **kwargs)"}, {"line": 1786, "name": "_call_impl", "filename": 4, "loc": "return forward_call(*args, **kwargs)"}, {"line": 804, "name": "compile_wrapper", "filename": 3, "loc": "return fn(*args, **kwargs)"}, {"line": 1775, "name": "_wrapped_call_impl", "filename": 4, "loc": "return self._call_impl(*args, **kwargs)"}, {"line": 1786, "name": "_call_impl", "filename": 4, "loc": "return forward_call(*args, **kwargs)"}, {"line": 74, "name": "forward", "filename": 5}]}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.209000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "54c68f2c93137a75d3220d7976663cbc"}
	{
	"name": "compile_attempt_0",
	"ts": 1757839469209879.5,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:29.214000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "16e33f547d5a5b6d4aec22b8ef294ace"}
	{
	"name": "bytecode_tracing",
	"ts": 1757839469214501.8,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:29.216000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 0, "describer_id": 0, "size": 65536}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.217000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 0, "ndim": 2, "dtype": "torch.int64", "device": "device(type='cuda', index=0)", "size": [1, 8192], "is_leaf": true, "stride": [8192, 1], "storage": 0, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Tensor object at 0x76eea6ea69e0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.217000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 0, "source": "L['idx']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.223000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 1, "describer_id": 0, "size": 2097152}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.223000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 1, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [8192, 128], "is_leaf": true, "stride": [128, 1], "storage": 1, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Tensor object at 0x76eef1577750>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.224000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 1, "source": "L['self']._buffers['cos']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.234000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 2, "describer_id": 0, "size": 2097152}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.234000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 3, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [8192, 128], "is_leaf": true, "stride": [128, 1], "storage": 2, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Tensor object at 0x76eef15777a0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.235000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 3, "source": "L['self']._buffers['sin']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.239000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 3, "describer_id": 0, "size": 1024000}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.240000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 4, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [256000, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 3, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef15776b0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.241000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 4, "source": "L['self']._modules['transformer']._modules['wte']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.275000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 5, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.276000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 14, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 5, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577840>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.276000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 14, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.288000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 6, "describer_id": 0, "size": 32768}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.288000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 17, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [8192, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 6, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef15777f0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.288000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 17, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.380000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 7, "describer_id": 0, "size": 16384}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.380000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 68, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2, 4096], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [4096, 1], "storage": 7, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577890>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.381000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 68, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.391000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 8, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.392000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 73, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 8, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577930>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.392000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 73, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.404000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 9, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.404000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 75, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 9, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577980>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.404000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 75, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.410000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 10, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.410000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 76, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [16, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 10, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef15778e0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.411000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 76, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.416000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 11, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.416000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 80, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [16, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 11, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef15779d0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.417000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 80, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.425000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 12, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.425000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 83, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2, 16], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [16, 1], "storage": 12, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577a20>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.426000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 83, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.435000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 13, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.435000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 87, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 13, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577ac0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.436000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 87, "source": "L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.466000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 14, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.466000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 88, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 14, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577b10>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.467000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 88, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.472000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 15, "describer_id": 0, "size": 32768}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.473000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 89, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [8192, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 15, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577a70>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.473000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 89, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.522000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 16, "describer_id": 0, "size": 16384}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.523000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 93, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2, 4096], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [4096, 1], "storage": 16, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577b60>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.523000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 93, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.531000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 17, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.532000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 94, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 17, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577c00>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.532000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 94, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.543000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 18, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.543000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 95, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 18, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577c50>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.544000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 95, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.549000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 19, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.549000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 96, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [16, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 19, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577bb0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.550000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 96, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.553000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 20, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.554000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 97, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [16, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 20, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577ca0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.554000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 97, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.559000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 21, "describer_id": 0, "size": 64}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.560000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 98, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2, 16], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [16, 1], "storage": 21, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577cf0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.560000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 98, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.568000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 22, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.568000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 99, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 22, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577d90>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.569000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 99, "source": "L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.583000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 23, "describer_id": 0, "size": 4}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.584000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 100, "ndim": 1, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [1], "storage": 23, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef1577d40>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.585000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 100, "source": "L['self']._modules['transformer']._modules['ln_f']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.588000 107386 torch/_subclasses/meta_utils.py:270] {"describe_storage": {"id": 24, "describer_id": 0, "size": 1024000}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.589000 107386 torch/_subclasses/meta_utils.py:487] {"describe_tensor": {"id": 101, "ndim": 2, "dtype": "torch.bfloat16", "device": "device(type='cuda', index=0)", "size": [256000, 2], "is_leaf": true, "requires_grad": true, "is_parameter": true, "stride": [2, 1], "storage": 24, "view_func": "_CustomViewFunc(func=<built-in method _view_func_unsafe of Parameter object at 0x76eef154abc0>)", "describer_id": 0}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.590000 107386 torch/_subclasses/meta_utils.py:1899] {"describe_source": {"describer_id": 0, "id": 101, "source": "L['self']._modules['lm_head']._parameters['weight']"}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:29.597000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "1333fc23b64bb17ba8ac182496e4b9bd"}
	{
	"name": "bytecode_tracing",
	"ts": 1757839469597611.5,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:29.609000 107386 torch/_dynamo/output_graph.py:1686] {"dynamo_output_graph": {"sizes": {"l_idx_": [1, 8192], "l_self_buffers_cos_": [8192, 128], "l_self_buffers_sin_": [8192, 128], "l_self_modules_transformer_modules_wte_parameters_weight_": [256000, 2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_": [8192, 2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_": [2, 4096], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_": [16, 2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_": [16, 2], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_": [2, 16], "l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_": [8192, 2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_": [2, 4096], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_": [2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_": [16, 2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_": [16, 2], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_": [2, 16], "l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_": [2], "l_self_modules_transformer_modules_ln_f_parameters_weight_": [2], "l_self_modules_lm_head_parameters_weight_": [256000, 2], "cos": [8192, 128], "sin": [8192, 128], "x": [1, 8192, 2], "tensor": [], "x_1": [1, 8192, 2], "x_2": [1, 8192, 2], "x_3": [1, 8192, 2], "x_4": [1, 8192, 2], "mul_1": [1, 8192, 2], "norm_x": [1, 8192, 1], "add": [1, 8192, 1], "rsqrt": [1, 8192, 1], "x_normed": [1, 8192, 2], "weight": [2], "float_2": [2], "mul_3": [1, 8192, 2], "x_5": [1, 8192, 2], "x_6": [1, 8192, 256000], "truediv": [1, 8192, 256000], "tanh": [1, 8192, 256000], "x_7": [1, 8192, 256000]}}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "8d4863a01a3dd34c32727caac55d5de3"}
	class GraphModule(torch.nn.Module):
	    def forward(self, L_idx_: "i64[1, 8192][8192, 1]cuda:0", L_self_buffers_cos_: "bf16[8192, 128][128, 1]cuda:0", L_self_buffers_sin_: "bf16[8192, 128][128, 1]cuda:0", L_self_modules_transformer_modules_wte_parameters_weight_: "bf16[256000, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_: "bf16[8192, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_: "bf16[2, 4096][4096, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_: "bf16[2, 16][16, 1]cuda:0", L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_: "bf16[8192, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_: "bf16[2, 4096][4096, 1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_: "bf16[2, 16][16, 1]cuda:0", L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_transformer_modules_ln_f_parameters_weight_: "bf16[2][1]cuda:0", L_self_modules_lm_head_parameters_weight_: "bf16[256000, 2][2, 1]cuda:0"):
	        l_idx_ = L_idx_
	        l_self_buffers_cos_ = L_self_buffers_cos_
	        l_self_buffers_sin_ = L_self_buffers_sin_
	        l_self_modules_transformer_modules_wte_parameters_weight_ = L_self_modules_transformer_modules_wte_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = L_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_
	        l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = L_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_
	        l_self_modules_transformer_modules_ln_f_parameters_weight_ = L_self_modules_transformer_modules_ln_f_parameters_weight_
	        l_self_modules_lm_head_parameters_weight_ = L_self_modules_lm_head_parameters_weight_
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:90 in forward, code: cos = self.cos[:T]
	        cos: "bf16[8192, 128][128, 1]cuda:0" = l_self_buffers_cos_[slice(None, 8192, None)];  l_self_buffers_cos_ = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:91 in forward, code: sin = self.sin[:T]
	        sin: "bf16[8192, 128][128, 1]cuda:0" = l_self_buffers_sin_[slice(None, 8192, None)];  l_self_buffers_sin_ = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:94 in forward, code: x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)
	        x: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = torch.nn.functional.embedding(l_idx_, l_self_modules_transformer_modules_wte_parameters_weight_, None, None, 2.0, False, False);  l_idx_ = l_self_modules_transformer_modules_wte_parameters_weight_ = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:96 in forward, code: x = x * torch.tensor(self.config.n_embd**0.5, dtype=x.dtype)
	        tensor: "bf16[][]cpu" = torch.tensor(1.4142135623730951, dtype = torch.bfloat16)
	        x_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = x * tensor;  x = tensor = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward, code: return self.checkpoint_fn(  # type: ignore[misc]
	        wrap_body_0 = self.wrap_body_0
	        tag_activation_checkpoint = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_0, x_1, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_, cos, sin, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_, use_reentrant = False);  wrap_body_0 = x_1 = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = None
	        x_2: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = tag_activation_checkpoint[0];  tag_activation_checkpoint = None
	        wrap_body_1 = self.wrap_body_1
	        tag_activation_checkpoint_1 = torch.ops.higher_order.tag_activation_checkpoint(wrap_body_1, x_2, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_, cos, sin, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_, use_reentrant = False);  wrap_body_1 = x_2 = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = cos = sin = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = None
	        x_3: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = tag_activation_checkpoint_1[0];  tag_activation_checkpoint_1 = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	        x_4: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_3.float();  x_3 = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	        mul_1: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_4 * x_4
	        norm_x: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_1, dim = -1, keepdim = True);  mul_1 = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	        add: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x + 1e-05;  norm_x = None
	        rsqrt: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	        x_normed: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_4 * rsqrt;  x_4 = rsqrt = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	        weight: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_ln_f_parameters_weight_;  l_self_modules_transformer_modules_ln_f_parameters_weight_ = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	        float_2: "f32[2][1]cuda:0" = weight.float();  weight = None
	        mul_3: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed * float_2;  x_normed = float_2 = None
	        x_5: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_3.to(dtype = torch.bfloat16);  mul_3 = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:101 in forward, code: x = self.lm_head(x)  # (b, t, vocab_size)
	        x_6: "bf16[1, 8192, 256000][2097152000, 256000, 1]cuda:0" = torch._C._nn.linear(x_5, l_self_modules_lm_head_parameters_weight_, None);  x_5 = l_self_modules_lm_head_parameters_weight_ = None
	        
	         # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:103 in forward, code: x = torch.tanh(x / self.config.final_logit_softcapping) * self.config.final_logit_softcapping
	        truediv: "bf16[1, 8192, 256000][2097152000, 256000, 1]cuda:0" = x_6 / 30.0;  x_6 = None
	        tanh: "bf16[1, 8192, 256000][2097152000, 256000, 1]cuda:0" = torch.tanh(truediv);  truediv = None
	        x_7: "bf16[1, 8192, 256000][2097152000, 256000, 1]cuda:0" = tanh * 30.0;  tanh = None
	        return (x_7,)
	        
	    class wrap_body_0(torch.nn.Module):
	        def forward(self, x_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_: "bf16[8192, 2][2, 1]cuda:0", cos: "bf16[8192, 128][128, 1]cuda:0", sin: "bf16[8192, 128][128, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_: "bf16[2, 4096][4096, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_: "bf16[2, 16][16, 1]cuda:0", l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_: "bf16[2][1]cuda:0"):
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_1.float()
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x * x
	            norm_x: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul, dim = -1, keepdim = True);  mul = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x + 1e-05;  norm_x = None
	            rsqrt: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	            x_normed: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x * rsqrt;  x = rsqrt = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_;  l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_2: "f32[2][1]cuda:0" = weight.float();  weight = None
	            mul_2: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed * float_2;  x_normed = float_2 = None
	            x_normed_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_2.to(dtype = torch.bfloat16);  mul_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:269 in forward, code: qkv = self.attn(x)
	            qkv: "bf16[1, 8192, 8192][67108864, 8192, 1]cuda:0" = torch._C._nn.linear(x_normed_1, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_, None);  x_normed_1 = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:274 in forward, code: qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
	            qkv_1: "bf16[1, 8192, 16, 4, 128][67108864, 8192, 512, 128, 1]cuda:0" = qkv.view(1, 8192, 16, 4, 128);  qkv = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:275 in forward, code: qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
	            qkv_2: "bf16[1, 16, 4, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = qkv_1.permute(0, 2, 3, 1, 4);  qkv_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:278 in forward, code: q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
	            split = qkv_2.split((2, 1, 1), dim = 2);  qkv_2 = None
	            q: "bf16[1, 16, 2, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[0]
	            k: "bf16[1, 16, 1, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[1]
	            v: "bf16[1, 16, 1, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[2];  split = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:284 in forward, code: k = k.expand(B, self.config.n_query_groups, q_per_kv, T, self.config.head_size)
	            k_1: "bf16[1, 16, 2, 8192, 128][67108864, 512, 0, 8192, 1]cuda:0" = k.expand(1, 16, 2, 8192, 128);  k = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:285 in forward, code: v = v.expand(B, self.config.n_query_groups, q_per_kv, T, self.config.head_size)
	            v_1: "bf16[1, 16, 2, 8192, 128][67108864, 512, 0, 8192, 1]cuda:0" = v.expand(1, 16, 2, 8192, 128);  v = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:287 in forward, code: q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
	            q_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = q.reshape(1, -1, 8192, 128);  q = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:288 in forward, code: k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
	            k_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = k_1.reshape(1, -1, 8192, 128);  k_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:289 in forward, code: v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
	            v_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = v_1.reshape(1, -1, 8192, 128);  v_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:291 in forward, code: q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
	            getitem_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = q_1[(Ellipsis, slice(None, 128, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:581 in apply_rope, code: x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
	            x1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_3[(Ellipsis, slice(None, 64, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:582 in apply_rope, code: x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
	            x2: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_3[(Ellipsis, slice(64, None, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:583 in apply_rope, code: rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
	            neg: "bf16[1, 32, 8192, 64][16777216, 524288, 64, 1]cuda:0" = -x2;  x2 = None
	            rotated: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:588 in apply_rope, code: cos = cos.unsqueeze(-3)
	            cos_1: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = cos.unsqueeze(-3)
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:589 in apply_rope, code: sin = sin.unsqueeze(-3)
	            sin_1: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = sin.unsqueeze(-3)
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:591 in apply_rope, code: roped = (x * cos) + (rotated * sin)
	            mul_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = getitem_3 * cos_1;  getitem_3 = cos_1 = None
	            mul_4: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = rotated * sin_1;  rotated = sin_1 = None
	            roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = mul_3 + mul_4;  mul_3 = mul_4 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:592 in apply_rope, code: return roped.to(dtype=x.dtype)
	            q_roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = roped.to(dtype = torch.bfloat16);  roped = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:292 in forward, code: k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
	            getitem_6: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = k_2[(Ellipsis, slice(None, 128, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:581 in apply_rope, code: x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
	            x1_1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_6[(Ellipsis, slice(None, 64, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:582 in apply_rope, code: x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
	            x2_1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_6[(Ellipsis, slice(64, None, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:583 in apply_rope, code: rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
	            neg_1: "bf16[1, 32, 8192, 64][16777216, 524288, 64, 1]cuda:0" = -x2_1;  x2_1 = None
	            rotated_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:588 in apply_rope, code: cos = cos.unsqueeze(-3)
	            cos_2: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = cos.unsqueeze(-3);  cos = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:589 in apply_rope, code: sin = sin.unsqueeze(-3)
	            sin_2: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = sin.unsqueeze(-3);  sin = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:591 in apply_rope, code: roped = (x * cos) + (rotated * sin)
	            mul_5: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = getitem_6 * cos_2;  getitem_6 = cos_2 = None
	            mul_6: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = rotated_1 * sin_2;  rotated_1 = sin_2 = None
	            roped_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = mul_5 + mul_6;  mul_5 = mul_6 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:592 in apply_rope, code: return roped.to(dtype=x.dtype)
	            k_roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = roped_1.to(dtype = torch.bfloat16);  roped_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:293 in forward, code: q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
	            getitem_9: "bf16[1, 32, 8192, 0][33554432, 1048576, 128, 1]cuda:0" = q_1[(Ellipsis, slice(128, None, None))];  q_1 = None
	            q_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((q_roped, getitem_9), dim = -1);  q_roped = getitem_9 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:294 in forward, code: k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
	            getitem_10: "bf16[1, 32, 8192, 0][33554432, 1048576, 128, 1]cuda:0" = k_2[(Ellipsis, slice(128, None, None))];  k_2 = None
	            k_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((k_roped, getitem_10), dim = -1);  k_roped = getitem_10 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:313 in forward, code: mask = torch.ones(T, T, dtype=q.dtype, device=q.device).triu(diagonal=1)
	            ones: "bf16[8192, 8192][8192, 1]cuda:0" = torch.ones(8192, 8192, dtype = torch.bfloat16, device = device(type='cuda', index=0))
	            mask: "bf16[8192, 8192][8192, 1]cuda:0" = ones.triu(diagonal = 1);  ones = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:314 in forward, code: mask.masked_fill_(mask.bool(), float("-inf"))
	            bool_1: "b8[8192, 8192][8192, 1]cuda:0" = mask.bool()
	            masked_fill_: "bf16[8192, 8192][8192, 1]cuda:0" = mask.masked_fill_(bool_1, -inf);  bool_1 = masked_fill_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:315 in forward, code: sliding_window_bias = torch.ones_like(mask).tril(diagonal=-self.config.sliding_window_size)
	            ones_like: "bf16[8192, 8192][8192, 1]cuda:0" = torch.ones_like(mask)
	            sliding_window_bias: "bf16[8192, 8192][8192, 1]cuda:0" = ones_like.tril(diagonal = -4096);  ones_like = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:316 in forward, code: sliding_window_bias.masked_fill_(sliding_window_bias.bool(), float("-inf"))
	            bool_2: "b8[8192, 8192][8192, 1]cuda:0" = sliding_window_bias.bool()
	            masked_fill__1: "bf16[8192, 8192][8192, 1]cuda:0" = sliding_window_bias.masked_fill_(bool_2, -inf);  bool_2 = masked_fill__1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:317 in forward, code: mask += sliding_window_bias
	            mask += sliding_window_bias;  mask_1: "bf16[8192, 8192][8192, 1]cuda:0" = mask;  mask = sliding_window_bias = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:334 in scaled_dot_product_attention, code: scores = q @ k.mT * scale
	            getattr_1: "bf16[1, 32, 128, 8192][33554432, 1048576, 1, 128]cuda:0" = k_3.mT;  k_3 = None
	            matmul: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = q_2 @ getattr_1;  q_2 = getattr_1 = None
	            scores: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = matmul * 0.08333333333333333;  matmul = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:336 in scaled_dot_product_attention, code: torch.tanh(scores / self.config.attention_logit_softcapping) * self.config.attention_logit_softcapping
	            truediv: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = scores / 50.0;  scores = None
	            tanh: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = torch.tanh(truediv);  truediv = None
	            scores_1: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = tanh * 50.0;  tanh = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:341 in scaled_dot_product_attention, code: scores = scores + mask
	            scores_2: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = scores_1 + mask_1;  scores_1 = mask_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:342 in scaled_dot_product_attention, code: scores = torch.nn.functional.softmax(scores, dim=-1, dtype=torch.float).to(dtype=q.dtype)
	            softmax: "f32[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = torch.nn.functional.softmax(scores_2, dim = -1, dtype = torch.float32);  scores_2 = None
	            scores_3: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = softmax.to(dtype = torch.bfloat16);  softmax = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:343 in scaled_dot_product_attention, code: y = scores @ v
	            y: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = scores_3 @ v_2;  scores_3 = v_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:348 in scaled_dot_product_attention, code: return y.transpose(1, 2)
	            y_1: "bf16[1, 8192, 32, 128][33554432, 128, 1048576, 1]cuda:0" = y.transpose(1, 2);  y = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:321 in forward, code: y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
	            y_2: "bf16[1, 8192, 4096][33554432, 4096, 1]cuda:0" = y_1.reshape(1, 8192, 4096);  y_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:324 in forward, code: return self.proj(y)
	            attention_output: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = torch._C._nn.linear(y_2, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_, None);  y_2 = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_2: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = attention_output.float();  attention_output = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_9: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_2 * x_2
	            norm_x_1: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_9, dim = -1, keepdim = True);  mul_9 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_5: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_1 + 1e-05;  norm_x_1 = None
	            rsqrt_1: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_5);  add_5 = None
	            x_normed_2: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_2 * rsqrt_1;  x_2 = rsqrt_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_1: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_;  l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_4: "f32[2][1]cuda:0" = weight_1.float();  weight_1 = None
	            mul_11: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_2 * float_4;  x_normed_2 = float_4 = None
	            attention_output_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_11.to(dtype = torch.bfloat16);  mul_11 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:236 in forward, code: x = attention_output + x
	            x_3: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = attention_output_1 + x_1;  attention_output_1 = x_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_4: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_3.float()
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_12: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_4 * x_4
	            norm_x_2: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_12, dim = -1, keepdim = True);  mul_12 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_8: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_2 + 1e-05;  norm_x_2 = None
	            rsqrt_2: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_8);  add_8 = None
	            x_normed_3: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_4 * rsqrt_2;  x_4 = rsqrt_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_2: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_;  l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_6: "f32[2][1]cuda:0" = weight_2.float();  weight_2 = None
	            mul_14: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_3 * float_6;  x_normed_3 = float_6 = None
	            to_5: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_14.to(dtype = torch.bfloat16);  mul_14 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:406 in forward, code: x_fc_1 = self.fc_1(x)
	            x_fc_1: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.linear(to_5, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:407 in forward, code: x_fc_2 = self.fc_2(x)
	            x_fc_2: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.linear(to_5, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_, None);  to_5 = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:408 in forward, code: x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2
	            gelu: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.gelu(x_fc_1, approximate = 'tanh');  x_fc_1 = None
	            x_5: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = gelu * x_fc_2;  gelu = x_fc_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:409 in forward, code: return self.proj(x)
	            linear_4: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = torch._C._nn.linear(x_5, l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_, None);  x_5 = l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_6: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = linear_4.float();  linear_4 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_16: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_6 * x_6
	            norm_x_3: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_16, dim = -1, keepdim = True);  mul_16 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_10: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_3 + 1e-05;  norm_x_3 = None
	            rsqrt_3: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_10);  add_10 = None
	            x_normed_4: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_6 * rsqrt_3;  x_6 = rsqrt_3 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_3: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_;  l_self_modules_transformer_modules_h_modules_0_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_8: "f32[2][1]cuda:0" = weight_3.float();  weight_3 = None
	            mul_18: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_4 * float_8;  x_normed_4 = float_8 = None
	            to_6: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_18.to(dtype = torch.bfloat16);  mul_18 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:237 in forward, code: x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x
	            x_7: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = to_6 + x_3;  to_6 = x_3 = None
	            return (x_7,)
	            
	    class wrap_body_1(torch.nn.Module):
	        def forward(self, x_2: "bf16[1, 8192, 2][16384, 2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_: "bf16[8192, 2][2, 1]cuda:0", cos: "bf16[8192, 128][128, 1]cuda:0", sin: "bf16[8192, 128][128, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_: "bf16[2, 4096][4096, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_: "bf16[2][1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_: "bf16[16, 2][2, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_: "bf16[2, 16][16, 1]cuda:0", l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_: "bf16[2][1]cuda:0"):
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_2.float()
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x * x
	            norm_x: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul, dim = -1, keepdim = True);  mul = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x + 1e-05;  norm_x = None
	            rsqrt: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
	            x_normed: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x * rsqrt;  x = rsqrt = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_;  l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_1_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_2: "f32[2][1]cuda:0" = weight.float();  weight = None
	            mul_2: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed * float_2;  x_normed = float_2 = None
	            x_normed_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_2.to(dtype = torch.bfloat16);  mul_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:269 in forward, code: qkv = self.attn(x)
	            qkv: "bf16[1, 8192, 8192][67108864, 8192, 1]cuda:0" = torch._C._nn.linear(x_normed_1, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_, None);  x_normed_1 = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_attn_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:274 in forward, code: qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)
	            qkv_1: "bf16[1, 8192, 16, 4, 128][67108864, 8192, 512, 128, 1]cuda:0" = qkv.view(1, 8192, 16, 4, 128);  qkv = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:275 in forward, code: qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
	            qkv_2: "bf16[1, 16, 4, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = qkv_1.permute(0, 2, 3, 1, 4);  qkv_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:278 in forward, code: q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)
	            split = qkv_2.split((2, 1, 1), dim = 2);  qkv_2 = None
	            q: "bf16[1, 16, 2, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[0]
	            k: "bf16[1, 16, 1, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[1]
	            v: "bf16[1, 16, 1, 8192, 128][67108864, 512, 128, 8192, 1]cuda:0" = split[2];  split = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:284 in forward, code: k = k.expand(B, self.config.n_query_groups, q_per_kv, T, self.config.head_size)
	            k_1: "bf16[1, 16, 2, 8192, 128][67108864, 512, 0, 8192, 1]cuda:0" = k.expand(1, 16, 2, 8192, 128);  k = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:285 in forward, code: v = v.expand(B, self.config.n_query_groups, q_per_kv, T, self.config.head_size)
	            v_1: "bf16[1, 16, 2, 8192, 128][67108864, 512, 0, 8192, 1]cuda:0" = v.expand(1, 16, 2, 8192, 128);  v = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:287 in forward, code: q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)
	            q_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = q.reshape(1, -1, 8192, 128);  q = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:288 in forward, code: k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)
	            k_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = k_1.reshape(1, -1, 8192, 128);  k_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:289 in forward, code: v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)
	            v_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = v_1.reshape(1, -1, 8192, 128);  v_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:291 in forward, code: q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)
	            getitem_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = q_1[(Ellipsis, slice(None, 128, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:581 in apply_rope, code: x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
	            x1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_3[(Ellipsis, slice(None, 64, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:582 in apply_rope, code: x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
	            x2: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_3[(Ellipsis, slice(64, None, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:583 in apply_rope, code: rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
	            neg: "bf16[1, 32, 8192, 64][16777216, 524288, 64, 1]cuda:0" = -x2;  x2 = None
	            rotated: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:588 in apply_rope, code: cos = cos.unsqueeze(-3)
	            cos_1: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = cos.unsqueeze(-3)
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:589 in apply_rope, code: sin = sin.unsqueeze(-3)
	            sin_1: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = sin.unsqueeze(-3)
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:591 in apply_rope, code: roped = (x * cos) + (rotated * sin)
	            mul_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = getitem_3 * cos_1;  getitem_3 = cos_1 = None
	            mul_4: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = rotated * sin_1;  rotated = sin_1 = None
	            roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = mul_3 + mul_4;  mul_3 = mul_4 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:592 in apply_rope, code: return roped.to(dtype=x.dtype)
	            q_roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = roped.to(dtype = torch.bfloat16);  roped = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:292 in forward, code: k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)
	            getitem_6: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = k_2[(Ellipsis, slice(None, 128, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:581 in apply_rope, code: x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)
	            x1_1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_6[(Ellipsis, slice(None, 64, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:582 in apply_rope, code: x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)
	            x2_1: "bf16[1, 32, 8192, 64][33554432, 1048576, 128, 1]cuda:0" = getitem_6[(Ellipsis, slice(64, None, None))]
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:583 in apply_rope, code: rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)
	            neg_1: "bf16[1, 32, 8192, 64][16777216, 524288, 64, 1]cuda:0" = -x2_1;  x2_1 = None
	            rotated_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:588 in apply_rope, code: cos = cos.unsqueeze(-3)
	            cos_2: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = cos.unsqueeze(-3);  cos = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:589 in apply_rope, code: sin = sin.unsqueeze(-3)
	            sin_2: "bf16[1, 8192, 128][1048576, 128, 1]cuda:0" = sin.unsqueeze(-3);  sin = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:591 in apply_rope, code: roped = (x * cos) + (rotated * sin)
	            mul_5: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = getitem_6 * cos_2;  getitem_6 = cos_2 = None
	            mul_6: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = rotated_1 * sin_2;  rotated_1 = sin_2 = None
	            roped_1: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = mul_5 + mul_6;  mul_5 = mul_6 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:592 in apply_rope, code: return roped.to(dtype=x.dtype)
	            k_roped: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = roped_1.to(dtype = torch.bfloat16);  roped_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:293 in forward, code: q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)
	            getitem_9: "bf16[1, 32, 8192, 0][33554432, 1048576, 128, 1]cuda:0" = q_1[(Ellipsis, slice(128, None, None))];  q_1 = None
	            q_2: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((q_roped, getitem_9), dim = -1);  q_roped = getitem_9 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:294 in forward, code: k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)
	            getitem_10: "bf16[1, 32, 8192, 0][33554432, 1048576, 128, 1]cuda:0" = k_2[(Ellipsis, slice(128, None, None))];  k_2 = None
	            k_3: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = torch.cat((k_roped, getitem_10), dim = -1);  k_roped = getitem_10 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:334 in scaled_dot_product_attention, code: scores = q @ k.mT * scale
	            getattr_1: "bf16[1, 32, 128, 8192][33554432, 1048576, 1, 128]cuda:0" = k_3.mT;  k_3 = None
	            matmul: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = q_2 @ getattr_1;  q_2 = getattr_1 = None
	            scores: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = matmul * 0.08333333333333333;  matmul = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:336 in scaled_dot_product_attention, code: torch.tanh(scores / self.config.attention_logit_softcapping) * self.config.attention_logit_softcapping
	            truediv: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = scores / 50.0;  scores = None
	            tanh: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = torch.tanh(truediv);  truediv = None
	            scores_1: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = tanh * 50.0;  tanh = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:339 in scaled_dot_product_attention, code: mask = torch.ones(q.size(2), q.size(2), dtype=q.dtype, device=q.device).triu(diagonal=1)
	            ones: "bf16[8192, 8192][8192, 1]cuda:0" = torch.ones(8192, 8192, dtype = torch.bfloat16, device = device(type='cuda', index=0))
	            mask: "bf16[8192, 8192][8192, 1]cuda:0" = ones.triu(diagonal = 1);  ones = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:340 in scaled_dot_product_attention, code: mask.masked_fill_(mask.bool(), torch.finfo(q.dtype).min)
	            bool_1: "b8[8192, 8192][8192, 1]cuda:0" = mask.bool()
	            masked_fill_: "bf16[8192, 8192][8192, 1]cuda:0" = mask.masked_fill_(bool_1, -3.3895313892515355e+38);  bool_1 = masked_fill_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:341 in scaled_dot_product_attention, code: scores = scores + mask
	            scores_2: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = scores_1 + mask;  scores_1 = mask = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:342 in scaled_dot_product_attention, code: scores = torch.nn.functional.softmax(scores, dim=-1, dtype=torch.float).to(dtype=q.dtype)
	            softmax: "f32[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = torch.nn.functional.softmax(scores_2, dim = -1, dtype = torch.float32);  scores_2 = None
	            scores_3: "bf16[1, 32, 8192, 8192][2147483648, 67108864, 8192, 1]cuda:0" = softmax.to(dtype = torch.bfloat16);  softmax = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:343 in scaled_dot_product_attention, code: y = scores @ v
	            y: "bf16[1, 32, 8192, 128][33554432, 1048576, 128, 1]cuda:0" = scores_3 @ v_2;  scores_3 = v_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:348 in scaled_dot_product_attention, code: return y.transpose(1, 2)
	            y_1: "bf16[1, 8192, 32, 128][33554432, 128, 1048576, 1]cuda:0" = y.transpose(1, 2);  y = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:321 in forward, code: y = y.reshape(B, T, self.config.head_size * self.config.n_head)  # re-assemble all head outputs side by side
	            y_2: "bf16[1, 8192, 4096][33554432, 4096, 1]cuda:0" = y_1.reshape(1, 8192, 4096);  y_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:324 in forward, code: return self.proj(y)
	            attention_output: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = torch._C._nn.linear(y_2, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_, None);  y_2 = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_attn_modules_proj_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_3: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = attention_output.float();  attention_output = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_9: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_3 * x_3
	            norm_x_1: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_9, dim = -1, keepdim = True);  mul_9 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_5: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_1 + 1e-05;  norm_x_1 = None
	            rsqrt_1: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_5);  add_5 = None
	            x_normed_2: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_3 * rsqrt_1;  x_3 = rsqrt_1 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_1: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_;  l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_attention_norm_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_4: "f32[2][1]cuda:0" = weight_1.float();  weight_1 = None
	            mul_11: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_2 * float_4;  x_normed_2 = float_4 = None
	            attention_output_1: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_11.to(dtype = torch.bfloat16);  mul_11 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:236 in forward, code: x = attention_output + x
	            x_4: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = attention_output_1 + x_2;  attention_output_1 = x_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_5: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_4.float()
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_12: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_5 * x_5
	            norm_x_2: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_12, dim = -1, keepdim = True);  mul_12 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_8: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_2 + 1e-05;  norm_x_2 = None
	            rsqrt_2: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_8);  add_8 = None
	            x_normed_3: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_5 * rsqrt_2;  x_5 = rsqrt_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_2: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_;  l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_norm_2_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_6: "f32[2][1]cuda:0" = weight_2.float();  weight_2 = None
	            mul_14: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_3 * float_6;  x_normed_3 = float_6 = None
	            to_5: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_14.to(dtype = torch.bfloat16);  mul_14 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:406 in forward, code: x_fc_1 = self.fc_1(x)
	            x_fc_1: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.linear(to_5, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_, None);  l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_1_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:407 in forward, code: x_fc_2 = self.fc_2(x)
	            x_fc_2: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.linear(to_5, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_, None);  to_5 = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_fc_2_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:408 in forward, code: x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2
	            gelu: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = torch._C._nn.gelu(x_fc_1, approximate = 'tanh');  x_fc_1 = None
	            x_6: "bf16[1, 8192, 16][131072, 16, 1]cuda:0" = gelu * x_fc_2;  gelu = x_fc_2 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:409 in forward, code: return self.proj(x)
	            linear_4: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = torch._C._nn.linear(x_6, l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_, None);  x_6 = l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_mlp_modules_proj_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:643 in forward, code: x = x.float()
	            x_7: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = linear_4.float();  linear_4 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:645 in forward, code: norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)
	            mul_16: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_7 * x_7
	            norm_x_3: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.mean(mul_16, dim = -1, keepdim = True);  mul_16 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:646 in forward, code: x_normed = x * torch.rsqrt(norm_x + self.eps)
	            add_10: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = norm_x_3 + 1e-05;  norm_x_3 = None
	            rsqrt_3: "f32[1, 8192, 1][8192, 1, 1]cuda:0" = torch.rsqrt(add_10);  add_10 = None
	            x_normed_4: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_7 * rsqrt_3;  x_7 = rsqrt_3 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:647 in forward, code: weight = (1 + self.weight) if self.add_unit_offset else self.weight
	            weight_3: "bf16[2][1]cuda:0" = 1 + l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_;  l_self_modules_transformer_modules_h_modules_1_modules_checkpoint_wrapped_module_modules_post_mlp_norm_parameters_weight_ = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:648 in forward, code: return (x_normed * weight.float()).to(dtype=dtype)
	            float_8: "f32[2][1]cuda:0" = weight_3.float();  weight_3 = None
	            mul_18: "f32[1, 8192, 2][16384, 2, 1]cuda:0" = x_normed_4 * float_8;  x_normed_4 = float_8 = None
	            to_6: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = mul_18.to(dtype = torch.bfloat16);  mul_18 = None
	            
	             # File: /usr/local/lib/python3.12/dist-packages/litgpt/model.py:237 in forward, code: x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x
	            x_8: "bf16[1, 8192, 2][16384, 2, 1]cuda:0" = to_6 + x_4;  to_6 = x_4 = None
	            return (x_8,)
	            
V0914 01:44:29.610000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "0a6090c6a9ee3e59cffeb56947579c61"}
	{
	"name": "backend_compile",
	"ts": 1757839469610616.8,
	"args": {
	"fn_name": "OutputGraph.call_user_compiler",
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.059000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "977fe23b795b82173c702d6562af59de"}
	{
	"name": "backend_compile",
	"ts": 1757839471059450.2,
	"args": {
	"fn_name": "OutputGraph.call_user_compiler",
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.065000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "d937e7f3406bc55a8318d9c8eb8826ad"}
	{
	"name": "compile_attempt_0",
	"ts": 1757839471065599.0,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.066000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "8ca4ebadef071c89c77e7cf609011d9f"}
	{
	"name": "build_guards",
	"ts": 1757839471066075.2,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.137000 107386 torch/_dynamo/guards.py:3308] {"dynamo_cpp_guards_str": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "c0832487c98597f336baffd5adb27bbd"}
	
	TREE_GUARD_MANAGER:
	+- RootGuardManager
	| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:643 in init_ambient_guards
	| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:631 in init_ambient_guards
	| +- GLOBAL_STATE: ___check_global_state()
	| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
	| +- GuardManager: source=L['idx'], accessed_by=FrameLocalsGuardAccessor(key='idx', framelocals_idx=1), type=<class 'torch.Tensor'>, tag_safe=(True, False)
	| | +- TENSOR_MATCH: check_tensor(L['idx'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[1, 8192], stride=[8192, 1])  # T = idx.size(1)  # litgpt/model.py:75 in forward
	| | +- NO_HASATTR: hasattr(L['idx'], '_dynamo_dynamic_indices') == False         # T = idx.size(1)  # litgpt/model.py:75 in forward
	| | +- NO_TENSOR_ALIASING: check_no_aliasing(L['idx'], L['self']._buffers['cos'], L['self']._buffers['sin'])
	| +- GuardManager: source=L['input_pos'], accessed_by=FrameLocalsGuardAccessor(key='input_pos', framelocals_idx=2), type=<class 'NoneType'>, tag_safe=(True, False)
	| | +- NONE_MATCH: L['input_pos'] is None                                        # if input_pos is not None:  # use the kv cache  # litgpt/model.py:79 in forward
	| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)
	| | +- GuardManager: source=G['math'], accessed_by=DictGetItemGuardAccessor('math'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['math'], 130786249730592)                   # scale = 1.0 / math.sqrt(self.config.attention_scores_scalar or self.config.head_size)  # litgpt/model.py:329 in scaled_dot_product_attention
	| | | +- GuardManager: source=G['math'].sqrt, accessed_by=GetAttrGuardAccessor(sqrt), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['math'].sqrt, 130786247874800)              # scale = 1.0 / math.sqrt(self.config.attention_scores_scalar or self.config.head_size)  # litgpt/model.py:329 in scaled_dot_product_attention
	| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['torch'], 130786247636720)                  # x = x * torch.tensor(self.config.n_embd**0.5, dtype=x.dtype)  # litgpt/model.py:96 in forward
	| | | +- GuardManager: source=G['torch'].nn, accessed_by=GetAttrGuardAccessor(nn), type=<class 'module'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn, 130782829636608)               # scores = torch.nn.functional.softmax(scores, dim=-1, dtype=torch.float).to(dtype=q.dtype)  # litgpt/model.py:342 in scaled_dot_product_attention
	| | | +- GuardManager: source=G['torch'].cat, accessed_by=GetAttrGuardAccessor(cat), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].cat, 130786242768592)              # rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)  # litgpt/model.py:583 in apply_rope
	| | | +- GuardManager: source=G['torch'].mean, accessed_by=GetAttrGuardAccessor(mean), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].mean, 130786242808224)             # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward
	| | | +- GuardManager: source=G['torch'].ones, accessed_by=GetAttrGuardAccessor(ones), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].ones, 130786242671808)             # mask = torch.ones(T, T, dtype=q.dtype, device=q.device).triu(diagonal=1)  # litgpt/model.py:313 in forward
	| | | +- GuardManager: source=G['torch'].tanh, accessed_by=GetAttrGuardAccessor(tanh), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].tanh, 130786242735024)             # torch.tanh(scores / self.config.attention_logit_softcapping) * self.config.attention_logit_softcapping  # litgpt/model.py:336 in scaled_dot_product_attention
	| | | +- GuardManager: source=G['torch'].finfo, accessed_by=GetAttrGuardAccessor(finfo), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].finfo, 130786054914688)            # mask.masked_fill_(mask.bool(), torch.finfo(q.dtype).min)  # litgpt/model.py:340 in scaled_dot_product_attention
	| | | +- GuardManager: source=G['torch'].float, accessed_by=GetAttrGuardAccessor(float), type=<class 'torch.dtype'>, tag_safe=(False, False)
	| | | | +- EQUALS_MATCH: G['torch'].float == torch.float32                             # scores = torch.nn.functional.softmax(scores, dim=-1, dtype=torch.float).to(dtype=q.dtype)  # litgpt/model.py:342 in scaled_dot_product_attention
	| | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 130786242733024)            # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | +- GuardManager: source=G['torch'].tensor, accessed_by=GetAttrGuardAccessor(tensor), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].tensor, 130786242659408)           # x = x * torch.tensor(self.config.n_embd**0.5, dtype=x.dtype)  # litgpt/model.py:96 in forward
	| | | +- GuardManager: source=G['torch'].ones_like, accessed_by=GetAttrGuardAccessor(ones_like), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['torch'].ones_like, 130786242731824)        # sliding_window_bias = torch.ones_like(mask).tril(diagonal=-self.config.sliding_window_size)  # litgpt/model.py:315 in forward
	| | +- GuardManager: source=G['apply_rope'], accessed_by=DictGetItemGuardAccessor('apply_rope'), type=<class 'function'>, tag_safe=(False, False)
	| | | +- GuardManager: source=G['apply_rope'].__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['apply_rope'].__code__, 130768660964912)    # q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)  # litgpt/model.py:291 in forward
	| | +- GuardManager: source=G['_130786247636720_c0'], accessed_by=DictGetItemGuardAccessor('_130786247636720_c0'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- GuardManager: source=G['_130786247636720_c0'].Tensor, accessed_by=GetAttrGuardAccessor(Tensor), type=<class 'torch._C._TensorMeta'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['_130786247636720_c0'].Tensor, 983226096)   # if is_namedtuple_class(node_type):  # utils/_pytree.py:1018 in _get_node_type
	| | +- GuardManager: source=G['__builtins_dict___0'], accessed_by=DictGetItemGuardAccessor('__builtins_dict___0'), type=<class 'dict'>, tag_safe=(False, False)
	| | | +- GuardManager: source=G['__builtins_dict___0']['len'], accessed_by=DictGetItemGuardAccessor('len'), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['len'], 130786250777168)  # if len(leaves) != self.num_leaves:  # utils/_pytree.py:1198 in unflatten
	| | | +- GuardManager: source=G['__builtins_dict___0']['iter'], accessed_by=DictGetItemGuardAccessor('iter'), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['iter'], 130786250777008)  # return iter(self._modules.values())  # nn/modules/container.py:405 in __iter__
	| | | +- GuardManager: source=G['__builtins_dict___0']['list'], accessed_by=DictGetItemGuardAccessor('list'), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['list'], 10729632)   # def helper(node: PyTree, leaves: list[Any]) -> TreeSpec:  # utils/_pytree.py:1276 in tree_flatten
	| | | +- GuardManager: source=G['__builtins_dict___0']['type'], accessed_by=DictGetItemGuardAccessor('type'), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['type'], 10767168)   # node_type = type(tree)  # utils/_pytree.py:1013 in _get_node_type
	| | | +- GuardManager: source=G['__builtins_dict___0']['float'], accessed_by=DictGetItemGuardAccessor('float'), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['float'], 10720960)  # mask.masked_fill_(mask.bool(), float("-inf"))  # litgpt/model.py:314 in forward
	| | | +- GuardManager: source=G['__builtins_dict___0']['super'], accessed_by=DictGetItemGuardAccessor('super'), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['super'], 10759936)  # return super().__getattr__(name)  # defer to nn.Module's logic  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:49 in __getattr__
	| | | +- GuardManager: source=G['__builtins_dict___0']['tuple'], accessed_by=DictGetItemGuardAccessor('tuple'), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['tuple'], 10759232)  # and issubclass(cls, tuple)  # utils/_pytree.py:683 in is_namedtuple_class
	| | | +- GuardManager: source=G['__builtins_dict___0']['isinstance'], accessed_by=DictGetItemGuardAccessor('isinstance'), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['isinstance'], 130786250776848)  # isinstance(cls, type)  # utils/_pytree.py:682 in is_namedtuple_class
	| | | +- GuardManager: source=G['__builtins_dict___0']['issubclass'], accessed_by=DictGetItemGuardAccessor('issubclass'), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___0']['issubclass'], 130786250776928)  # and issubclass(cls, tuple)  # utils/_pytree.py:683 in is_namedtuple_class
	| | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_utils_dot__pytree'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree'], 130782644817408)  # def helper(node: PyTree, leaves: list[Any]) -> TreeSpec:  # utils/_pytree.py:1276 in tree_flatten
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].Any, accessed_by=GetAttrGuardAccessor(Any), type=<class 'typing._AnyMeta'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree'].Any, 900260592)  # def helper(node: PyTree, leaves: list[Any]) -> TreeSpec:  # utils/_pytree.py:1276 in tree_flatten
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].TreeSpec, accessed_by=GetAttrGuardAccessor(TreeSpec), type=<class 'type'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree'].TreeSpec, 986494128)  # if not isinstance(treespec, TreeSpec):  # utils/_pytree.py:1298 in tree_unflatten
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC, accessed_by=GetAttrGuardAccessor(_LEAF_SPEC), type=<class 'torch.utils._pytree.LeafSpec'>, tag_safe=(False, False)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC, 988582192)  # if not isinstance(treespec, TreeSpec):  # utils/_pytree.py:1298 in tree_unflatten
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | +- DICT_CONTAINS: not ___dict_contains('unflatten', G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.__dict__)  # return treespec.unflatten(leaves)  # utils/_pytree.py:1303 in tree_unflatten
	| | | | | +- DICT_CONTAINS: not ___dict_contains('is_leaf', G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.__dict__)  # if self.is_leaf():  # utils/_pytree.py:1204 in unflatten
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.num_nodes, accessed_by=GetAttrGuardAccessor(num_nodes), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.num_nodes == 1  # return self.num_nodes == 1 and self.num_leaves == 1  # utils/_pytree.py:1110 in is_leaf
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.num_leaves, accessed_by=GetAttrGuardAccessor(num_leaves), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: G['__import_torch_dot_utils_dot__pytree']._LEAF_SPEC.num_leaves == 1  # if len(leaves) != self.num_leaves:  # utils/_pytree.py:1198 in unflatten
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].tree_is_leaf, accessed_by=GetAttrGuardAccessor(tree_is_leaf), type=<class 'function'>, tag_safe=(False, False)
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].tree_is_leaf.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree'].tree_is_leaf.__code__, 130782644754224)  # if tree_is_leaf(node, is_leaf=is_leaf):  # utils/_pytree.py:1277 in helper
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._get_node_type, accessed_by=GetAttrGuardAccessor(_get_node_type), type=<class 'function'>, tag_safe=(False, False)
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree']._get_node_type.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree']._get_node_type.__code__, 130782644753968)  # return _get_node_type(tree) not in SUPPORTED_NODES  # utils/_pytree.py:1045 in tree_is_leaf
	| | | +- DictGuardManager: source=G['__import_torch_dot_utils_dot__pytree'].SUPPORTED_NODES, accessed_by=GetAttrGuardAccessor(SUPPORTED_NODES), type=<class 'dict'>, tag_safe=(True, False)
	| | | | +- DICT_VERSION: ___dict_version(G['__import_torch_dot_utils_dot__pytree'].SUPPORTED_NODES) == 4  # return _get_node_type(tree) not in SUPPORTED_NODES  # utils/_pytree.py:1045 in tree_is_leaf
	| | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].is_namedtuple_class, accessed_by=GetAttrGuardAccessor(is_namedtuple_class), type=<class 'function'>, tag_safe=(False, False)
	| | | | +- GuardManager: source=G['__import_torch_dot_utils_dot__pytree'].is_namedtuple_class.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_utils_dot__pytree'].is_namedtuple_class.__code__, 130782741312208)  # if is_namedtuple_class(node_type):  # utils/_pytree.py:1018 in _get_node_type
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_linear'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 130782697783216)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_module'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 130782829724272)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks), type=<class 'collections.OrderedDict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 10741888)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks), type=<class 'collections.OrderedDict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 10741888)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks), type=<class 'collections.OrderedDict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 10741888)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks), type=<class 'collections.OrderedDict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 10741888)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_sparse'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'], 130782200801024)  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F, accessed_by=GetAttrGuardAccessor(F), type=<class 'module'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F, 130782697783136)  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.gelu, accessed_by=GetAttrGuardAccessor(gelu), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.gelu, 130783183408320)  # x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2  # litgpt/model.py:408 in forward
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.linear, accessed_by=GetAttrGuardAccessor(linear), type=<class 'builtin_function_or_method'>, tag_safe=(True, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.linear, 130783183409440)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.softmax, accessed_by=GetAttrGuardAccessor(softmax), type=<class 'function'>, tag_safe=(False, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.softmax, 130782244207680)  # scores = torch.nn.functional.softmax(scores, dim=-1, dtype=torch.float).to(dtype=q.dtype)  # litgpt/model.py:342 in scaled_dot_product_attention
	| | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.embedding, accessed_by=GetAttrGuardAccessor(embedding), type=<class 'function'>, tag_safe=(False, False)
	| | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_sparse'].F.embedding, 130782244274880)  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | +- GuardManager: source=G['__import_torch_dot_distributed_dot_algorithms_dot__checkpoint_dot_checkpoint_wrapper'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_distributed_dot_algorithms_dot__checkpoint_dot_checkpoint_wrapper'), type=<class 'module'>, tag_safe=(False, False)
	| | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_distributed_dot_algorithms_dot__checkpoint_dot_checkpoint_wrapper'], 130786245224832)  # if self.checkpoint_impl == CheckpointImpl.REENTRANT and kwargs != {}:  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:149 in forward
	| | | +- GuardManager: source=G['__import_torch_dot_distributed_dot_algorithms_dot__checkpoint_dot_checkpoint_wrapper'].CheckpointImpl, accessed_by=GetAttrGuardAccessor(CheckpointImpl), type=<class 'enum.EnumType'>, tag_safe=(False, False)
	| | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_distributed_dot_algorithms_dot__checkpoint_dot_checkpoint_wrapper'].CheckpointImpl, 903033248)  # if self.checkpoint_impl == CheckpointImpl.REENTRANT and kwargs != {}:  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:149 in forward
	| +- GuardManager: source=L['self'], accessed_by=FrameLocalsGuardAccessor(key='self', framelocals_idx=0), type=<class 'litgpt.model.GPT'>, tag_safe=(False, False)
	| | +- TYPE_MATCH: ___check_type_id(L['self'], 1123669648)                       # if self.max_seq_length < T:  # litgpt/model.py:76 in forward
	| | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)
	| | | +- DICT_CONTAINS: not ___dict_contains('max_seq_length', L['self'].__dict__)    # if self.max_seq_length < T:  # litgpt/model.py:76 in forward
	| | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor('config'), type=<class 'litgpt.config.Config'>, tag_safe=(False, False)
	| | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 1123642896)                # if self.config.scale_embeddings:  # litgpt/model.py:95 in forward
	| | | | +- GuardManager: source=L['self'].config.n_embd, accessed_by=GetAttrGuardAccessor(n_embd), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.n_embd == 2                                  # x = x * torch.tensor(self.config.n_embd**0.5, dtype=x.dtype)  # litgpt/model.py:96 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.head_size, accessed_by=GetAttrGuardAccessor(head_size), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.head_size == 128                             # qkv = qkv.view(B, T, self.config.n_query_groups, total_qkv, self.config.head_size)  # litgpt/model.py:274 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.rope_n_elem, accessed_by=GetAttrGuardAccessor(rope_n_elem), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.rope_n_elem == 128                           # q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)  # litgpt/model.py:291 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.n_query_groups, accessed_by=GetAttrGuardAccessor(n_query_groups), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.n_query_groups == 16                         # q_per_kv = self.config.n_head // self.config.n_query_groups  # litgpt/model.py:272 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.gelu_approximate, accessed_by=GetAttrGuardAccessor(gelu_approximate), type=<class 'str'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.gelu_approximate == 'tanh'                   # x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2  # litgpt/model.py:408 in forward
	| | | | +- GuardManager: source=L['self'].config.scale_embeddings, accessed_by=GetAttrGuardAccessor(scale_embeddings), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | +- TRUE_MATCH: L['self'].config.scale_embeddings == True                     # if self.config.scale_embeddings:  # litgpt/model.py:95 in forward
	| | | | +- GuardManager: source=L['self'].config.parallel_residual, accessed_by=GetAttrGuardAccessor(parallel_residual), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | +- FALSE_MATCH: L['self'].config.parallel_residual == False                   # if self.config.parallel_residual:  # litgpt/model.py:232 in forward
	| | | | +- GuardManager: source=L['self'].config.sliding_window_size, accessed_by=GetAttrGuardAccessor(sliding_window_size), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.sliding_window_size == 4096                  # sliding_window_bias = torch.ones_like(mask).tril(diagonal=-self.config.sliding_window_size)  # litgpt/model.py:315 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.attention_scores_scalar, accessed_by=GetAttrGuardAccessor(attention_scores_scalar), type=<class 'int'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.attention_scores_scalar == 144               # scale = 1.0 / math.sqrt(self.config.attention_scores_scalar or self.config.head_size)  # litgpt/model.py:329 in scaled_dot_product_attention (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | +- GuardManager: source=L['self'].config.final_logit_softcapping, accessed_by=GetAttrGuardAccessor(final_logit_softcapping), type=<class 'float'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.final_logit_softcapping == 30.0              # if self.config.final_logit_softcapping is not None:  # litgpt/model.py:102 in forward
	| | | | +- GuardManager: source=L['self'].config.attention_logit_softcapping, accessed_by=GetAttrGuardAccessor(attention_logit_softcapping), type=<class 'float'>, tag_safe=(True, False)
	| | | | | +- EQUALS_MATCH: L['self'].config.attention_logit_softcapping == 50.0          # if self.config.attention_logit_softcapping is not None:  # litgpt/model.py:332 in scaled_dot_product_attention
	| | | | +- GuardManager: source=type(L['self'].config), accessed_by=TypeGuardAccessor, type=<class 'type'>, tag_safe=(False, False)
	| | | | | +- GuardManager: source=dict(type(L['self'].config).__dict__), accessed_by=TypeDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | +- GuardManager: source=dict(type(L['self'].config).__dict__)['n_head'], accessed_by=DictGetItemGuardAccessor('n_head'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | +- EQUALS_MATCH: dict(type(L['self'].config).__dict__)['n_head'] == 32         # q_per_kv = self.config.n_head // self.config.n_query_groups  # litgpt/model.py:272 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | +- GuardManager: source=L['self']._buffers, accessed_by=DictGetItemGuardAccessor('_buffers'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(L['self']._buffers, 10737728)                # cos = self.cos[:T]  # litgpt/model.py:90 in forward
	| | | | +- GuardManager: source=L['self']._buffers['cos'], accessed_by=DictGetItemGuardAccessor('cos'), type=<class 'torch.Tensor'>, tag_safe=(True, False)
	| | | | | +- TENSOR_MATCH: check_tensor(L['self']._buffers['cos'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=False, size=[8192, 128], stride=[128, 1])  # cos = self.cos[:T]  # litgpt/model.py:90 in forward
	| | | | | +- NO_TENSOR_ALIASING
	| | | | +- GuardManager: source=L['self']._buffers['sin'], accessed_by=DictGetItemGuardAccessor('sin'), type=<class 'torch.Tensor'>, tag_safe=(True, False)
	| | | | | +- TENSOR_MATCH: check_tensor(L['self']._buffers['sin'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=False, size=[8192, 128], stride=[128, 1])  # sin = self.sin[:T]  # litgpt/model.py:91 in forward
	| | | | | +- NO_TENSOR_ALIASING
	| | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(False, False)
	| | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules, 10737728)                # cos = self.cos[:T]  # litgpt/model.py:90 in forward
	| | | | +- GuardManager: source=L['self']._modules['transformer'], accessed_by=DictGetItemGuardAccessor('transformer'), type=<class 'torch.nn.modules.container.ModuleDict'>, tag_safe=(False, False)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer'], 989406688)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | | | +- GuardManager: source=L['self']._modules['transformer'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)
	| | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(False, False)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules, 10737728)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h'], accessed_by=DictGetItemGuardAccessor('h'), type=<class 'torch.nn.modules.container.ModuleList'>, tag_safe=(False, False)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h'], 989405696)  # for block in self.transformer.h:  # litgpt/model.py:98 in forward
	| | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)
	| | | | | | | | | +- DictGuardManager: source=L['self']._modules['transformer']._modules['h']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(False, False)
	| | | | | | | | | | +- KeyValueManager pair at index=0
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['transformer']._modules['h']._modules))[0], type=<class 'str'>, tag_safe=(True, False)
	| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['transformer']._modules['h']._modules))[0] == '0'  # return iter(self._modules.values())  # nn/modules/container.py:405 in __iter__
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'], type=<class 'torch.distributed.algorithms._checkpoint.checkpoint_wrapper.CheckpointWrapper'>, tag_safe=(False, False)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0'], 903056912)  # x = block(x, cos, sin, mask, input_pos)  # litgpt/model.py:99 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0'].__dict__)  # x = block(x, cos, sin, mask, input_pos)  # litgpt/model.py:99 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._buffers, accessed_by=GetAttrGuardAccessor(_buffers), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._buffers, 10737728)  # if name in _buffers:  # nn/modules/module.py:1958 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['0']._buffers)  # if name in _buffers:  # nn/modules/module.py:1958 in __getattr__
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules, accessed_by=GetAttrGuardAccessor(_modules), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules, 10737728)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['0']._modules)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module'], accessed_by=DictGetItemGuardAccessor('_checkpoint_wrapped_module'), type=<class 'litgpt.model.Block'>, tag_safe=(True, True)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module'], 1123665248)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module'].__dict__)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules, 10737728)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp'), type=<class 'litgpt.model.GemmaMLP'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp'], 1123727136)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules, 10737728)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'], accessed_by=DictGetItemGuardAccessor('fc_1'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'], 989884096)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'].__dict__)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[16, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'], accessed_by=DictGetItemGuardAccessor('fc_2'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'], 989884096)  # x_fc_2 = self.fc_2(x)  # litgpt/model.py:407 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'].__dict__)  # x_fc_2 = self.fc_2(x)  # litgpt/model.py:407 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[16, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'], accessed_by=DictGetItemGuardAccessor('proj'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'], 989884096)  # return self.proj(x)  # litgpt/model.py:409 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'].__dict__)  # return self.proj(x)  # litgpt/model.py:409 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2, 16], stride=[16, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp']._parameters, 10737728)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'], accessed_by=DictGetItemGuardAccessor('attn'), type=<class 'litgpt.model.CausalSelfAttention'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'], 1123024272)  # attention_output = self.attn(x_normed, cos, sin, mask, input_pos)  # litgpt/model.py:229 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__)  # attention_output = self.attn(x_normed, cos, sin, mask, input_pos)  # litgpt/model.py:229 in forward
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('scaled_dot_product_attention', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__)  # y = self.scaled_dot_product_attention(q, k, v, mask)  # litgpt/model.py:319 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules, 10737728)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'], accessed_by=DictGetItemGuardAccessor('attn'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'], 989884096)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'].__dict__)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[8192, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'], accessed_by=DictGetItemGuardAccessor('proj'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'], 989884096)  # return self.proj(y)  # litgpt/model.py:324 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'].__dict__)  # return self.proj(y)  # litgpt/model.py:324 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2, 4096], stride=[4096, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn']._parameters, 10737728)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].apply_sliding_window_attention, accessed_by=DictGetItemGuardAccessor('apply_sliding_window_attention'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].apply_sliding_window_attention == True  # if self.apply_sliding_window_attention:  # litgpt/model.py:301 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'], accessed_by=DictGetItemGuardAccessor('norm_1'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'], 1123733248)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].__dict__)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_1'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'], accessed_by=DictGetItemGuardAccessor('norm_2'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'], 1123733248)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['norm_2'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'], accessed_by=DictGetItemGuardAccessor('post_mlp_norm'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'], 1123733248)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'], accessed_by=DictGetItemGuardAccessor('post_attention_norm'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'], 1123733248)  # attention_output = self.post_attention_norm(attention_output)  # litgpt/model.py:230 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].__dict__)  # attention_output = self.post_attention_norm(attention_output)  # litgpt/model.py:230 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._parameters, 10737728)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0']._parameters, accessed_by=GetAttrGuardAccessor(_parameters), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0']._parameters, 10737728)  # if name in _parameters:  # nn/modules/module.py:1954 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['0']._parameters)  # if name in _parameters:  # nn/modules/module.py:1954 in __getattr__
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn, accessed_by=GetAttrGuardAccessor(checkpoint_fn), type=<class 'functools.partial'>, tag_safe=(False, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn, 899978112)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.args, accessed_by=GetAttrGuardAccessor(args), type=<class 'tuple'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.args, 10759232)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | +- LENGTH_CHECK: not L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.args  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.keywords, accessed_by=GetAttrGuardAccessor(keywords), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.keywords) == 1  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.keywords['use_reentrant'], accessed_by=DictGetItemGuardAccessor('use_reentrant'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | +- FALSE_MATCH: L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_fn.keywords['use_reentrant'] == False  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_impl, accessed_by=GetAttrGuardAccessor(checkpoint_impl), type=<enum 'CheckpointImpl'>, tag_safe=(False, False)
	| | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['transformer']._modules['h']._modules['0'].checkpoint_impl, 130786245333968)  # if self.checkpoint_impl == CheckpointImpl.REENTRANT and kwargs != {}:  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:149 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].__getattr__, accessed_by=GetAttrGuardAccessor(__getattr__), type=<class 'method'>, tag_safe=(False, False)
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].__getattr__.__closure__, accessed_by=ClosureGuardAccessor, type=<class 'tuple'>, tag_safe=(False, False)
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].__getattr__.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0), type=<class 'cell'>, tag_safe=(False, False)
	| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['0'].__getattr__.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents), type=<class 'abc.ABCMeta'>, tag_safe=(False, False)
	| | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['transformer']._modules['h']._modules['0'].__getattr__.__closure__[0].cell_contents, 903000944)  # return super().__getattr__(name)  # defer to nn.Module's logic  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:49 in __getattr__
	| | | | | | | | | | +- KeyValueManager pair at index=1
	| | | | | | | | | | | +- KeyManager: GuardManager: source=list(dict.keys(L['self']._modules['transformer']._modules['h']._modules))[1], type=<class 'str'>, tag_safe=(True, False)
	| | | | | | | | | | | | +- EQUALS_MATCH: list(dict.keys(L['self']._modules['transformer']._modules['h']._modules))[1] == '1'  # return iter(self._modules.values())  # nn/modules/container.py:405 in __iter__
	| | | | | | | | | | | +- ValueManager: GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'], type=<class 'torch.distributed.algorithms._checkpoint.checkpoint_wrapper.CheckpointWrapper'>, tag_safe=(False, False)
	| | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1'], 903056912)  # x = block(x, cos, sin, mask, input_pos)  # litgpt/model.py:99 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1'].__dict__)  # x = block(x, cos, sin, mask, input_pos)  # litgpt/model.py:99 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._buffers, accessed_by=GetAttrGuardAccessor(_buffers), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._buffers, 10737728)  # if name in _buffers:  # nn/modules/module.py:1958 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['1']._buffers)  # if name in _buffers:  # nn/modules/module.py:1958 in __getattr__
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules, accessed_by=GetAttrGuardAccessor(_modules), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules, 10737728)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['1']._modules)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module'], accessed_by=DictGetItemGuardAccessor('_checkpoint_wrapped_module'), type=<class 'litgpt.model.Block'>, tag_safe=(True, True)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module'], 1123665248)  # if name in modules:  # nn/modules/module.py:1962 in __getattr__
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module'].__dict__)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules, 10737728)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp'], accessed_by=DictGetItemGuardAccessor('mlp'), type=<class 'litgpt.model.GemmaMLP'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp'], 1123727136)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules, 10737728)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'], accessed_by=DictGetItemGuardAccessor('fc_1'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'], 989884096)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1'].__dict__)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_1']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[16, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'], accessed_by=DictGetItemGuardAccessor('fc_2'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'], 989884096)  # x_fc_2 = self.fc_2(x)  # litgpt/model.py:407 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2'].__dict__)  # x_fc_2 = self.fc_2(x)  # litgpt/model.py:407 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['fc_2']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[16, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'], accessed_by=DictGetItemGuardAccessor('proj'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'], 989884096)  # return self.proj(x)  # litgpt/model.py:409 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj'].__dict__)  # return self.proj(x)  # litgpt/model.py:409 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._modules['proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2, 16], stride=[16, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp']._parameters, 10737728)  # x_fc_1 = self.fc_1(x)  # litgpt/model.py:406 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'], accessed_by=DictGetItemGuardAccessor('attn'), type=<class 'litgpt.model.CausalSelfAttention'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'], 1123024272)  # attention_output = self.attn(x_normed, cos, sin, mask, input_pos)  # litgpt/model.py:229 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('scaled_dot_product_attention', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__)  # y = self.scaled_dot_product_attention(q, k, v, mask)  # litgpt/model.py:319 in forward
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].__dict__)  # attention_output = self.attn(x_normed, cos, sin, mask, input_pos)  # litgpt/model.py:229 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules, accessed_by=DictGetItemGuardAccessor('_modules'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules, 10737728)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'], accessed_by=DictGetItemGuardAccessor('attn'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'], 989884096)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn'].__dict__)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['attn']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[8192, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'], accessed_by=DictGetItemGuardAccessor('proj'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'], 989884096)  # return self.proj(y)  # litgpt/model.py:324 in forward
	| | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj'].__dict__)  # return self.proj(y)  # litgpt/model.py:324 in forward
	| | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['bias'] is None  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._modules['proj']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2, 4096], stride=[4096, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn']._parameters, 10737728)  # qkv = self.attn(x)  # litgpt/model.py:269 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].apply_sliding_window_attention, accessed_by=DictGetItemGuardAccessor('apply_sliding_window_attention'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- FALSE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].apply_sliding_window_attention == False  # if self.apply_sliding_window_attention:  # litgpt/model.py:301 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'], accessed_by=DictGetItemGuardAccessor('norm_1'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'], 1123733248)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].__dict__)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_1'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'], accessed_by=DictGetItemGuardAccessor('norm_2'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'], 1123733248)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['norm_2'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'], accessed_by=DictGetItemGuardAccessor('post_mlp_norm'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'], 1123733248)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].__dict__)  # x = self.post_mlp_norm(self.mlp(self.norm_2(x))) + x  # litgpt/model.py:237 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_mlp_norm'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'], accessed_by=DictGetItemGuardAccessor('post_attention_norm'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'], 1123733248)  # attention_output = self.post_attention_norm(attention_output)  # litgpt/model.py:230 in forward
	| | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].__dict__)  # attention_output = self.post_attention_norm(attention_output)  # litgpt/model.py:230 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['post_attention_norm'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._parameters, 10737728)  # x_normed = self.norm_1(x)  # litgpt/model.py:228 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1']._parameters, accessed_by=GetAttrGuardAccessor(_parameters), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1']._parameters, 10737728)  # if name in _parameters:  # nn/modules/module.py:1954 in __getattr__
	| | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('_checkpoint_wrapped_module', L['self']._modules['transformer']._modules['h']._modules['1']._parameters)  # if name in _parameters:  # nn/modules/module.py:1954 in __getattr__
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn, accessed_by=GetAttrGuardAccessor(checkpoint_fn), type=<class 'functools.partial'>, tag_safe=(False, False)
	| | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn, 899978112)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.args, accessed_by=GetAttrGuardAccessor(args), type=<class 'tuple'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.args, 10759232)  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | +- LENGTH_CHECK: not L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.args  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.keywords, accessed_by=GetAttrGuardAccessor(keywords), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | +- DICT_LENGTH: len(L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.keywords) == 1  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.keywords['use_reentrant'], accessed_by=DictGetItemGuardAccessor('use_reentrant'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | | | | | | +- FALSE_MATCH: L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_fn.keywords['use_reentrant'] == False  # return self.checkpoint_fn(  # type: ignore[misc]  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:171 in forward
	| | | | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_impl, accessed_by=GetAttrGuardAccessor(checkpoint_impl), type=<enum 'CheckpointImpl'>, tag_safe=(False, False)
	| | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self']._modules['transformer']._modules['h']._modules['1'].checkpoint_impl, 130786245333968)  # if self.checkpoint_impl == CheckpointImpl.REENTRANT and kwargs != {}:  # distributed/algorithms/_checkpoint/checkpoint_wrapper.py:149 in forward
	| | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f'], accessed_by=DictGetItemGuardAccessor('ln_f'), type=<class 'litgpt.model.RMSNorm'>, tag_safe=(True, True)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['ln_f'], 1123733248)  # x = self.transformer.ln_f(x)  # litgpt/model.py:100 in forward
	| | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['ln_f'].__dict__)  # x = self.transformer.ln_f(x)  # litgpt/model.py:100 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f'].dim, accessed_by=DictGetItemGuardAccessor('dim'), type=<class 'int'>, tag_safe=(True, False)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['ln_f'].dim == -1  # norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)  # litgpt/model.py:645 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f'].eps, accessed_by=DictGetItemGuardAccessor('eps'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['ln_f'].eps == 1e-05  # x_normed = x * torch.rsqrt(norm_x + self.eps)  # litgpt/model.py:646 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['ln_f']._parameters, 10737728)  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['ln_f']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[2], stride=[1])  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['ln_f'].add_unit_offset, accessed_by=DictGetItemGuardAccessor('add_unit_offset'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | +- TRUE_MATCH: L['self']._modules['transformer']._modules['ln_f'].add_unit_offset == True  # weight = (1 + self.weight) if self.add_unit_offset else self.weight  # litgpt/model.py:647 in forward
	| | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'], accessed_by=DictGetItemGuardAccessor('wte'), type=<class 'torch.nn.modules.sparse.Embedding'>, tag_safe=(True, True)
	| | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['wte'], 991326688)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['transformer']._modules['wte'].__dict__)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].sparse, accessed_by=DictGetItemGuardAccessor('sparse'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | +- FALSE_MATCH: L['self']._modules['transformer']._modules['wte'].sparse == False  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].max_norm, accessed_by=DictGetItemGuardAccessor('max_norm'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['wte'].max_norm is None  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].norm_type, accessed_by=DictGetItemGuardAccessor('norm_type'), type=<class 'float'>, tag_safe=(True, False)
	| | | | | | | | | | +- EQUALS_MATCH: L['self']._modules['transformer']._modules['wte'].norm_type == 2.0  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._modules['wte']._parameters, 10737728)  # self.weight,  # nn/modules/sparse.py:194 in forward
	| | | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['transformer']._modules['wte']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[256000, 2], stride=[2, 1])  # self.weight,  # nn/modules/sparse.py:194 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].padding_idx, accessed_by=DictGetItemGuardAccessor('padding_idx'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | | | +- NONE_MATCH: L['self']._modules['transformer']._modules['wte'].padding_idx is None  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | | | | | | +- GuardManager: source=L['self']._modules['transformer']._modules['wte'].scale_grad_by_freq, accessed_by=DictGetItemGuardAccessor('scale_grad_by_freq'), type=<class 'bool'>, tag_safe=(True, False)
	| | | | | | | | | | +- FALSE_MATCH: L['self']._modules['transformer']._modules['wte'].scale_grad_by_freq == False  # return F.embedding(  # nn/modules/sparse.py:192 in forward
	| | | | | | +- GuardManager: source=L['self']._modules['transformer']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['transformer']._parameters, 10737728)  # x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)  # litgpt/model.py:94 in forward
	| | | | +- GuardManager: source=L['self']._modules['lm_head'], accessed_by=DictGetItemGuardAccessor('lm_head'), type=<class 'torch.nn.modules.linear.Linear'>, tag_safe=(True, True)
	| | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['lm_head'], 989884096)    # x = self.lm_head(x)  # (b, t, vocab_size)  # litgpt/model.py:101 in forward
	| | | | | +- GuardManager: source=L['self']._modules['lm_head'].__dict__, accessed_by=GetGenericDictGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['lm_head'].__dict__)  # x = self.lm_head(x)  # (b, t, vocab_size)  # litgpt/model.py:101 in forward
	| | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['lm_head']._parameters, 10737728)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias'), type=<class 'NoneType'>, tag_safe=(True, False)
	| | | | | | | | +- NONE_MATCH: L['self']._modules['lm_head']._parameters['bias'] is None     # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | | | | | +- GuardManager: source=L['self']._modules['lm_head']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight'), type=<class 'torch.nn.parameter.Parameter'>, tag_safe=(True, False)
	| | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['lm_head']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.bfloat16, device=0, requires_grad=True, size=[256000, 2], stride=[2, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	| | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters'), type=<class 'dict'>, tag_safe=(True, False)
	| | | | +- TYPE_MATCH: ___check_type_id(L['self']._parameters, 10737728)             # cos = self.cos[:T]  # litgpt/model.py:90 in forward
	| | | +- GuardManager: source=L['self']._max_seq_length, accessed_by=DictGetItemGuardAccessor('_max_seq_length'), type=<class 'int'>, tag_safe=(True, False)
	| | | | +- EQUALS_MATCH: L['self']._max_seq_length == 8192                             # if self.max_seq_length < T:  # litgpt/model.py:76 in forward (HINT: torch.compile considers integer attributes of the nn.Module to be static. If you are observing recompilation, you might want to make this integer dynamic using torch._dynamo.config.allow_unspec_int_on_nn_module = True, or convert this integer into a tensor.)
	+- LAMBDA_GUARD: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['torch'].nn.functional  # x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2  # litgpt/model.py:408 in forward
	+- LAMBDA_GUARD: G['__import_torch_dot_nn_dot_modules_dot_sparse'].F is G['__import_torch_dot_nn_dot_modules_dot_linear'].F  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:134 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module'].config  # if self.config.parallel_residual:  # litgpt/model.py:232 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module'].config  # if self.config.parallel_residual:  # litgpt/model.py:232 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['mlp'].config  # x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2  # litgpt/model.py:408 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['mlp'].config  # x = torch.nn.functional.gelu(x_fc_1, approximate=self.config.gelu_approximate) * x_fc_2  # litgpt/model.py:408 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['0']._modules['_checkpoint_wrapped_module']._modules['attn'].config  # q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)  # litgpt/model.py:293 in forward
	+- LAMBDA_GUARD: L['self'].config is L['self']._modules['transformer']._modules['h']._modules['1']._modules['_checkpoint_wrapped_module']._modules['attn'].config  # torch.tanh(scores / self.config.attention_logit_softcapping) * self.config.attention_logit_softcapping  # litgpt/model.py:336 in scaled_dot_product_attention
	
	Guard latency = 93.68 us
V0914 01:44:31.138000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f6d8bcdcb61592cfdf1e83787064b7dd"}
	{
	"name": "build_guards",
	"ts": 1757839471138728.5,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.139000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "402c89ab376e610039ce86d6d72db633"}
	{
	"name": "gc",
	"ts": 1757839471139291.0,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "B",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.140000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "f0499953a405a09ce6d295c2fe24a48a"}
	{
	"name": "gc",
	"ts": 1757839471140112.0,
	"args": {
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.140000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "73b7ba51f66328f9e243e89a536993ff"}
	{
	"name": "entire_frame_compile",
	"ts": 1757839471140613.0,
	"args": {
	"fn_name": "_compile.compile_inner",
	"compile_id": "0/0"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
V0914 01:44:31.142000 107386 torch/_dynamo/utils.py:1626] {"compilation_metrics": {"compile_id": "0/0", "frame_key": "1", "co_name": "forward", "co_filename": "/usr/local/lib/python3.12/dist-packages/litgpt/model.py", "co_firstlineno": 74, "cache_size": 0, "accumulated_cache_size": 0, "guard_count": 483, "shape_env_guard_count": 0, "graph_op_count": 23, "graph_node_count": 50, "graph_input_count": 24, "start_time": 1757839469.206104, "entire_frame_compile_time_s": 1.93449, "backend_compile_time_s": 1.448833, "inductor_compile_time_s": null, "code_gen_time_s": null, "fail_type": null, "fail_reason": null, "fail_user_frame_filename": null, "fail_user_frame_lineno": null, "non_compliant_ops": [], "compliant_custom_ops": [], "restart_reasons": [], "dynamo_time_before_restart_s": 0.0, "has_guarded_code": true, "remote_cache_time_saved_s": null, "structured_logging_overhead_s": 0.052364, "config_suppress_errors": false, "config_inline_inbuilt_nn_modules": true, "specialize_float": false, "dynamo_config": "{\"_autograd_backward_strict_mode_conditional_banned_ops\": [\"stride\", \"storage_offset\", \"is_contiguous\"], \"_unsafe_skip_fsdp_module_guards\": false, \"accumulated_recompile_limit\": 256, \"allow_complex_guards_as_runtime_asserts\": false, \"allow_empty_graphs\": false, \"allow_ignore_mark_dynamic\": false, \"allow_rnn\": false, \"allow_unspec_int_on_nn_module\": false, \"allowed_functions_module_string_ignorelist\": [\"torch._decomp\", \"torch._prims\", \"torch._refs\", \"torch.distributions\", \"torch.testing\"], \"assume_static_by_default\": true, \"automatic_dynamic_local_pgo\": true, \"automatic_dynamic_remote_pgo\": null, \"automatic_dynamic_shapes\": true, \"automatic_dynamic_shapes_mark_as\": \"dynamic\", \"caching_precompile\": false, \"capture_autograd_function\": true, \"capture_dynamic_output_shape_ops\": false, \"capture_func_transforms\": true, \"capture_scalar_outputs\": false, \"capture_sparse_compute\": true, \"compiled_autograd\": false, \"compiled_autograd_kwargs_override\": {}, \"cprofile\": false, \"cudagraph_backend_keep_input_mutation\": false, \"cudagraph_backend_support_input_mutation\": false, \"dead_code_elimination\": true, \"disable\": false, \"do_not_emit_runtime_asserts\": false, \"dont_skip_tracing\": false, \"dynamic_shapes\": true, \"enable_compiler_collectives\": false, \"enable_cpp_framelocals_guard_eval\": true, \"enable_cpp_guard_manager\": true, \"enable_cpp_symbolic_shape_guards\": true, \"enable_faithful_generator_behavior\": true, \"enable_trace_contextlib\": true, \"enable_trace_unittest\": false, \"error_on_nested_fx_trace\": true, \"error_on_nested_jit_trace\": false, \"error_on_recompile\": false, \"fail_on_recompile_limit_hit\": false, \"fake_tensor_cache_crosscheck_enabled\": false, \"fake_tensor_cache_enabled\": true, \"fake_tensor_disable_inference_mode\": true, \"force_nn_module_property_static_shapes\": true, \"force_parameter_static_shapes\": true, \"force_unspec_int_unbacked_size_like_on_torchrec_kjt\": false, \"graph_break_on_nn_param_ctor\": true, \"graph_deduplication_lint\": false, \"guard_nn_modules\": true, \"guard_nn_modules_using_dict_tags\": true, \"inline_inbuilt_nn_modules\": true, \"install_free_tensors\": false, \"issue_3_13_0_warning\": true, \"max_saved_pointers_for_recursive_dict_tags_check\": 256, \"minimum_call_count\": 1, \"numpy_default_complex\": \"complex128\", \"numpy_default_float\": \"float64\", \"numpy_default_int\": \"int64\", \"only_allow_pt2_compliant_ops\": false, \"optimize_ddp\": true, \"optimize_ddp_lazy_compile\": false, \"prefer_deferred_runtime_asserts_over_guards\": false, \"prepare_freezing\": false, \"pt2_compile_id_prefix\": null, \"raise_on_ctx_manager_usage\": true, \"raise_on_unsafe_aot_autograd\": false, \"recompile_limit\": 8, \"record_compile_time_instruction_count\": false, \"record_runtime_overhead\": true, \"replay_record_enabled\": false, \"report_guard_failures\": true, \"rewrite_assert_with_torch_assert\": true, \"run_gc_after_compile\": true, \"skip_code_recursive_on_recompile_limit_hit\": true, \"skip_fsdp_guards\": true, \"skip_fsdp_hooks\": true, \"skip_guards_on_constant_func_defaults\": true, \"skip_nnmodule_hook_guards\": true, \"skip_no_tensor_aliasing_guards_on_parameters\": true, \"skip_tensor_guards_with_matching_dict_tags\": true, \"skip_torchrec\": true, \"skipfiles_inline_module_allowlist\": {}, \"specialize_float\": false, \"specialize_int\": false, \"suppress_errors\": false, \"trace_numpy\": true, \"track_nodes_for_deduplication\": false, \"use_graph_deduplication\": false, \"use_lamba_guard_for_object_aliasing\": true, \"use_lazy_graph_module\": true, \"use_numpy_random_stream\": false, \"use_recursive_dict_tags_for_guards\": true, \"verify_correctness\": false, \"wrap_top_frame\": false}", "is_forward": true, "num_triton_bundles": null, "remote_fx_graph_cache_get_time_ms": null, "remote_fx_graph_cache_put_time_ms": null, "start_time_us": 1757839469206104, "duration_us": 1934490, "dynamo_cumulative_compile_time_us": 1934490, "aot_autograd_cumulative_compile_time_us": 1448833, "inductor_cumulative_compile_time_us": null, "inductor_code_gen_cumulative_compile_time_us": null, "triton_compile_time_us": null, "runtime_cudagraphify_time_us": null, "runtime_triton_autotune_time_us": null, "dynamo_compile_time_before_restart_us": 0, "distributed_ephemeral_timeout_us": null, "structured_logging_overhead_us": 52364, "remote_fx_graph_cache_get_time_us": null, "remote_fx_graph_cache_put_time_us": null, "backward_cumulative_compile_time_us": null, "end_time_us": 1757839471141023, "pre_grad_pass_time_us": null, "post_grad_pass_time_us": null, "joint_graph_pass_time_us": null, "log_format_version": 3, "inductor_config": "{\"TYPE_CHECKING\": false, \"_cache_config_ignore_prefix\": [\"trace\", \"cuda.cutlass_dir\", \"worker_start_method\", \"compile_threads\", \"post_grad_custom_post_pass\", \"post_grad_custom_pre_pass\", \"joint_custom_pre_pass\", \"joint_custom_post_pass\", \"_fuse_ddp_communication_passes\", \"_pre_fusion_custom_pass\", \"always_complex_memory_overlap_TESTING_ONLY\", \"fx_graph_cache\", \"fx_graph_remote_cache\", \"autotune_local_cache\", \"autotune_remote_cache\"], \"_collective.auto_select\": false, \"_collective.one_shot_all_reduce_threshold_bytes\": 131072, \"_fuse_ddp_bucket_size\": 25, \"_fuse_ddp_communication\": false, \"_fuse_ddp_communication_passes\": [\"fuse_ddp_with_concat_op\", \"schedule_comm_wait\"], \"_micro_pipeline_tp\": false, \"_post_fusion_custom_pass\": null, \"_pre_fusion_custom_pass\": null, \"_profile_var\": \"\", \"_raise_error_for_testing\": false, \"_save_config_ignore\": [\"trace.upload_tar\", \"joint_custom_pre_pass\", \"joint_custom_post_pass\", \"pre_grad_custom_pass\", \"aot_inductor.repro_level\", \"aot_inductor.dump_aoti_minifier\", \"post_grad_custom_pre_pass\", \"post_grad_custom_post_pass\", \"_fuse_ddp_communication_passes\", \"_pre_fusion_custom_pass\"], \"add_pre_grad_passes\": null, \"aggressive_fusion\": false, \"alignment_asserts\": true, \"allow_buffer_reuse\": true, \"always_complex_memory_overlap_TESTING_ONLY\": false, \"always_keep_tensor_constants\": false, \"annotate_training\": false, \"aot_inductor.allow_stack_allocation\": false, \"aot_inductor.compile_standalone\": false, \"aot_inductor.compile_wrapper_opt_level\": \"O1\", \"aot_inductor.custom_op_libs\": null, \"aot_inductor.custom_ops_to_c_shims\": {}, \"aot_inductor.debug_compile\": false, \"aot_inductor.debug_intermediate_value_printer\": \"0\", \"aot_inductor.dump_aoti_minifier\": false, \"aot_inductor.embed_kernel_binary\": false, \"aot_inductor.emit_multi_arch_kernel\": false, \"aot_inductor.enable_lto\": false, \"aot_inductor.filtered_kernel_names\": null, \"aot_inductor.force_mmap_weights\": false, \"aot_inductor.metadata\": {}, \"aot_inductor.model_name_for_generated_files\": null, \"aot_inductor.output_path\": \"\", \"aot_inductor.package\": false, \"aot_inductor.package_constants_in_so\": true, \"aot_inductor.package_constants_on_disk\": false, \"aot_inductor.package_cpp_only\": null, \"aot_inductor.precompile_headers\": true, \"aot_inductor.presets\": {}, \"aot_inductor.raise_error_on_ignored_optimization\": true, \"aot_inductor.repro_level\": 2, \"aot_inductor.serialized_in_spec\": \"\", \"aot_inductor.serialized_out_spec\": \"\", \"aot_inductor.use_consts_asm_build\": true, \"aot_inductor.use_minimal_arrayref_interface\": false, \"aot_inductor.use_runtime_constant_folding\": false, \"aot_inductor.weight_use_caching_allocator\": false, \"assert_indirect_indexing\": true, \"assume_aligned_inputs\": false, \"assume_unaligned_fallback_output\": false, \"autoheuristic_collect\": \"\", \"autoheuristic_log_path\": \"DEFAULT\", \"autoheuristic_use\": \"mixed_mm\", \"autotune_fallback_to_aten\": false, \"autotune_in_subproc\": false, \"autotune_local_cache\": true, \"autotune_lookup_table\": {}, \"autotune_multi_device\": false, \"autotune_num_choices_displayed\": 10, \"autotune_remote_cache\": null, \"b2b_gemm_pass\": false, \"batch_fusion\": true, \"benchmark_combo_kernel\": false, \"benchmark_epilogue_fusion\": true, \"benchmark_fusion\": false, \"benchmark_harness\": true, \"benchmark_kernel\": false, \"bfloat16_atomic_adds_enabled\": true, \"bucket_all_gathers_fx\": \"none\", \"bucket_all_gathers_fx_bucket_size_determinator\": null, \"bucket_reduce_scatters_fx\": \"none\", \"bucket_reduce_scatters_fx_bucket_size_determinator\": null, \"bundle_triton_into_fx_graph_cache\": true, \"bundled_autotune_remote_cache\": null, \"bw_outputs_user_visible\": true, \"can_inplace_pad_graph_input\": false, \"check_stack_no_cycles_TESTING_ONLY\": false, \"combo_kernel_allow_mixed_sizes\": 1, \"combo_kernel_foreach_dynamic_shapes\": true, \"combo_kernels\": false, \"combo_kernels_autotune\": 1, \"comment_origin\": false, \"compile_threads\": 32, \"comprehensive_padding\": true, \"compute_all_bounds\": false, \"constant_and_index_propagation\": true, \"conv_1x1_as_mm\": false, \"coordinate_descent_check_all_directions\": false, \"coordinate_descent_search_radius\": 1, \"coordinate_descent_tuning\": false, \"cpp.cxx\": [null, \"g++\"], \"cpp.descriptive_names\": \"original_aten\", \"cpp.dynamic_threads\": false, \"cpp.enable_concat_linear\": false, \"cpp.enable_floating_point_contract_flag\": \"off\", \"cpp.enable_grouped_gemm_template\": false, \"cpp.enable_kernel_profile\": false, \"cpp.enable_loop_tail_vec\": true, \"cpp.enable_tiling_heuristics\": true, \"cpp.enable_unsafe_math_opt_flag\": false, \"cpp.fallback_scatter_reduce_sum\": true, \"cpp.force_inline_kernel\": false, \"cpp.gemm_cache_blocking\": null, \"cpp.gemm_max_k_slices\": 1, \"cpp.gemm_thread_factors\": null, \"cpp.inject_log1p_bug_TESTING_ONLY\": null, \"cpp.inject_relu_bug_TESTING_ONLY\": null, \"cpp.max_horizontal_fusion_size\": 16, \"cpp.min_chunk_size\": 512, \"cpp.no_redundant_loops\": true, \"cpp.simdlen\": null, \"cpp.threads\": -1, \"cpp.use_decompose_tanh\": false, \"cpp.use_small_dequant_buffer\": false, \"cpp.vec_isa_ok\": null, \"cpp.weight_prepack\": true, \"cpp_cache_precompile_headers\": true, \"cpp_wrapper\": false, \"cpp_wrapper_build_separate\": false, \"cpu_backend\": \"cpp\", \"cuda.arch\": null, \"cuda.binary_remote_cache_force_write\": false, \"cuda.compile_opt_level\": \"-O1\", \"cuda.cuda_cxx\": null, \"cuda.cutlass_backend_min_gemm_size\": 1, \"cuda.cutlass_dir\": \"/opt/pytorch/pytorch/third_party/cutlass\", \"cuda.cutlass_enabled_ops\": \"all\", \"cuda.cutlass_epilogue_fusion_enabled\": false, \"cuda.cutlass_hash_with_compile_cmd\": false, \"cuda.cutlass_instantiation_level\": \"0\", \"cuda.cutlass_max_profiling_configs\": null, \"cuda.cutlass_max_profiling_swizzle_options\": [1, 2, 4, 8], \"cuda.cutlass_op_allowlist_regex\": null, \"cuda.cutlass_op_denylist_regex\": null, \"cuda.cutlass_prescreening\": true, \"cuda.cutlass_presets\": null, \"cuda.cutlass_tma_only\": false, \"cuda.enable_caching_codegen\": true, \"cuda.enable_cuda_lto\": false, \"cuda.enable_debug_info\": false, \"cuda.enable_ptxas_info\": false, \"cuda.generate_test_runner\": false, \"cuda.upload_to_binary_remote_cache\": false, \"cuda.use_binary_remote_cache\": true, \"cuda.use_fast_math\": false, \"cuda.version\": null, \"cuda_backend\": \"triton\", \"dce\": false, \"debug\": false, \"debug_fusion\": false, \"debug_index_asserts\": false, \"debug_ir_traceback\": false, \"decompose_mem_bound_mm\": false, \"developer_warnings\": false, \"disable_cpp_codegen\": false, \"disable_padding_cpu\": true, \"disable_progress\": true, \"dynamic_scale_rblock\": true, \"efficient_conv_bn_eval_fx_passes\": false, \"emulate_precision_casts\": false, \"enable_auto_functionalized_v2\": true, \"enable_caching_generated_triton_templates\": true, \"enable_linear_binary_folding\": false, \"enabled_metric_tables\": \"\", \"epilogue_fusion\": true, \"epilogue_fusion_first\": false, \"estimate_op_runtime\": \"default\", \"external_matmul\": [], \"fallback_random\": false, \"force_fuse_int_mm_with_mul\": false, \"force_layout_optimization\": false, \"force_pointwise_cat\": false, \"force_same_precision\": false, \"force_shape_pad\": false, \"freezing\": false, \"freezing_discard_parameters\": false, \"fx_graph_cache\": true, \"fx_graph_remote_cache\": null, \"fx_passes_numeric_check\": {\"num_iterations\": 1, \"pre_grad\": false, \"precision\": 0.0001, \"requires_optimizer\": true}, \"generate_intermediate_hooks\": false, \"global_cache_dir\": null, \"graph_partition\": false, \"group_fusion\": false, \"halide.asserts\": false, \"halide.cpu_target\": \"host\", \"halide.debug\": false, \"halide.gpu_target\": \"host-cuda\", \"halide.scan_kernels\": false, \"halide.scheduler_cpu\": \"Adams2019\", \"halide.scheduler_cuda\": \"Anderson2021\", \"implicit_fallbacks\": true, \"inplace_buffers\": true, \"inplace_padding\": true, \"inter_node_bw\": 25, \"intra_node_bw\": 300, \"is_nightly_or_source\": false, \"is_predispatch\": false, \"joint_custom_post_pass\": null, \"joint_custom_pre_pass\": null, \"joint_graph_constant_folding\": true, \"keep_output_stride\": true, \"kernel_name_max_ops\": 10, \"layout_opt_default\": \"1\", \"layout_optimization\": true, \"loop_ordering_after_fusion\": false, \"max_autotune\": false, \"max_autotune_conv_backends\": \"ATEN,TRITON\", \"max_autotune_flex_search_space\": \"DEFAULT\", \"max_autotune_gemm\": false, \"max_autotune_gemm_backends\": \"ATEN,TRITON,CPP\", \"max_autotune_gemm_search_space\": \"DEFAULT\", \"max_autotune_pointwise\": false, \"max_autotune_report_choices_stats\": true, \"max_autotune_subproc_graceful_timeout_seconds\": 0.0, \"max_autotune_subproc_result_timeout_seconds\": 60.0, \"max_autotune_subproc_terminate_timeout_seconds\": 0.0, \"max_epilogue_benchmarked_choices\": 1, \"max_fusion_buffer_group_pairwise_attempts\": 64, \"max_fusion_size\": 64, \"max_pointwise_cat_inputs\": 8, \"memory_planning\": false, \"memory_pool\": \"intermediates\", \"min_num_split\": 0, \"mixed_mm_choice\": \"heuristic\", \"multi_kernel_hints\": [], \"nan_asserts\": false, \"non_blocking_remote_cache_write\": true, \"online_softmax\": true, \"optimize_scatter_upon_const_tensor\": true, \"pad_channels_last\": false, \"pad_outputs\": false, \"padding_alignment_bytes\": 128, \"padding_stride_threshold\": 1024, \"pattern_matcher\": true, \"permute_fusion\": false, \"pick_loop_orders\": true, \"post_grad_custom_post_pass\": null, \"post_grad_custom_pre_pass\": null, \"post_grad_fusion_options\": {}, \"pre_grad_custom_pass\": null, \"pre_grad_fusion_options\": {}, \"precompilation_timeout_seconds\": 3600, \"profile_bandwidth\": false, \"profile_bandwidth_output\": null, \"profile_bandwidth_regex\": \"\", \"profile_bandwidth_with_do_bench_using_profiling\": false, \"profiler_mark_wrapper_call\": false, \"prologue_fusion\": true, \"quiesce_async_compile_pool\": false, \"realize_acc_reads_size_threshold\": null, \"realize_acc_reads_threshold\": 8, \"realize_opcount_threshold\": 30, \"realize_reads_threshold\": 4, \"remove_pre_grad_passes\": null, \"reorder_for_compute_comm_overlap\": false, \"reorder_for_compute_comm_overlap_passes\": [\"reorder_compute_for_overlap\", \"sink_waits\", \"raise_comms\"], \"reorder_for_locality\": true, \"reorder_for_peak_memory\": true, \"reorder_prefetch_limit\": null, \"rocm.arch\": [], \"rocm.ck_dir\": null, \"rocm.ck_max_profiling_configs\": null, \"rocm.ck_supported_arch\": [\"gfx90a\", \"gfx942\", \"gfx950\"], \"rocm.ck_tile_max_profiling_configs\": null, \"rocm.compile_opt_level\": \"-O2\", \"rocm.flush_denormals\": true, \"rocm.generate_test_runner\": false, \"rocm.is_debug\": false, \"rocm.kBatch_sweep\": null, \"rocm.n_max_profiling_configs\": null, \"rocm.print_kernel_resource_usage\": false, \"rocm.rocm_home\": null, \"rocm.save_temps\": false, \"rocm.split_k_threshold\": 16, \"rocm.use_fast_math\": true, \"rocm.use_preselected_instances\": false, \"save_args\": false, \"scalar_asserts\": true, \"score_fusion_memory_threshold\": 10, \"search_autotune_cache\": false, \"shape_padding\": true, \"size_asserts\": true, \"sleep_sec_TESTING_ONLY\": null, \"split_cat_fx_passes\": true, \"split_reductions\": true, \"static_launch_user_defined_triton_kernels\": false, \"static_weight_shapes\": true, \"strict_static_cuda_launcher\": false, \"test_configs.autotune_choice_desc_regex\": null, \"test_configs.autotune_choice_name_regex\": null, \"test_configs.force_extern_kernel_in_multi_template\": false, \"test_configs.graphsafe_rng_func_ignores_fallback_random\": false, \"test_configs.max_mm_configs\": null, \"test_configs.runtime_triton_dtype_assert\": false, \"test_configs.static_cpp_dtype_assert\": false, \"trace.compile_profile\": false, \"trace.debug_dir\": null, \"trace.debug_log\": false, \"trace.dot_graph_shape\": null, \"trace.draw_orig_fx_graph\": false, \"trace.enabled\": false, \"trace.fx_graph\": true, \"trace.fx_graph_transformed\": true, \"trace.graph_diagram\": false, \"trace.info_log\": false, \"trace.ir_post_fusion\": true, \"trace.ir_pre_fusion\": true, \"trace.log_autotuning_results\": false, \"trace.log_url_for_graph_xform\": null, \"trace.output_code\": true, \"trace.provenance_tracking\": false, \"trace.save_real_tensors\": false, \"trace.upload_tar\": null, \"triton.autotune_at_compile_time\": null, \"triton.autotune_cublasLt\": true, \"triton.autotune_pointwise\": true, \"triton.autotune_with_sample_inputs\": false, \"triton.coalesce_tiling_analysis\": true, \"triton.codegen_upcast_to_fp32\": true, \"triton.cooperative_reductions\": false, \"triton.cudagraph_capture_sizes\": null, \"triton.cudagraph_dynamic_shape_warn_limit\": 50, \"triton.cudagraph_skip_dynamic_graphs\": false, \"triton.cudagraph_support_input_mutation\": true, \"triton.cudagraph_trees\": true, \"triton.cudagraph_trees_history_recording\": false, \"triton.cudagraph_unexpected_rerecord_limit\": 128, \"triton.cudagraphs\": false, \"triton.debug_sync_graph\": false, \"triton.debug_sync_kernel\": false, \"triton.decompose_k_threshold\": 32, \"triton.dense_indexing\": false, \"triton.descriptive_names\": \"original_aten\", \"triton.disallow_failing_autotune_kernels_TESTING_ONLY\": false, \"triton.divisible_by_16\": true, \"triton.enable_persistent_tma_matmul\": false, \"triton.fast_path_cudagraph_asserts\": false, \"triton.force_cooperative_reductions\": false, \"triton.force_cudagraph_sync\": false, \"triton.force_cudagraphs_warmup\": false, \"triton.inject_relu_bug_TESTING_ONLY\": null, \"triton.max_tiles\": null, \"triton.min_split_scan_rblock\": 256, \"triton.multi_kernel\": 0, \"triton.num_decompose_k_splits\": 10, \"triton.persistent_reductions\": true, \"triton.prefer_nd_tiling\": false, \"triton.skip_cudagraph_warmup\": false, \"triton.skip_l1_cache\": false, \"triton.slow_path_cudagraph_asserts\": true, \"triton.spill_threshold\": 16, \"triton.store_cubin\": false, \"triton.tile_reductions\": false, \"triton.tiling_prevents_pointwise_fusion\": true, \"triton.tiling_prevents_reduction_fusion\": true, \"triton.unique_kernel_names\": true, \"triton.unique_user_kernel_names\": false, \"triton.use_block_ptr\": false, \"triton.use_tensor_descriptor\": false, \"triton_kernel_default_layout_constraint\": \"needs_fixed_stride_order\", \"unbacked_symint_fallback\": 8192, \"unroll_reductions_threshold\": 8, \"unsafe_ignore_unsupported_triton_autotune_args\": false, \"unsafe_marked_cacheable_functions\": {}, \"unsafe_skip_cache_dynamic_shape_guards\": false, \"use_experimental_benchmarker\": true, \"use_fast_math\": false, \"use_mixed_mm\": true, \"use_static_cuda_launcher\": true, \"verbose_progress\": false, \"warn_mix_layout\": false, \"worker_start_method\": \"subprocess\", \"worker_suppress_logging\": true}", "remote_cache_version": null, "inductor_fx_remote_cache_hit_count": null, "inductor_fx_remote_cache_miss_count": null, "inductor_fx_remote_cache_backend_type": null, "inductor_fx_remote_cache_hit_keys": null, "inductor_fx_remote_cache_miss_keys": null, "cuda_version": "13.0", "triton_version": "3.4.0", "feature_usage": null, "compile_time_autotune_time_us": null, "is_runtime": false, "gc_time_us": 821, "tensorify_float_attempt": null, "tensorify_float_success": null, "tensorify_float_failure": null, "guard_latency_us": 93, "recompile_reason": null, "num_graph_breaks": null, "triton_kernel_compile_times_us": null, "ir_count": 1273, "cudagraph_skip_reason": null, "python_version": "3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]", "pgo_put_remote_code_state_time_us": null, "pgo_get_remote_code_state_time_us": null, "param_numel": 1073362, "param_bytes": 2146724, "param_count": 21, "recompile_user_contexts": null}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0}
V0914 01:44:31.143000 107386 torch/_dynamo/utils.py:1931] {"chromium_event": {}, "frame_id": 0, "frame_compile_id": 0, "attempt": 0, "has_payload": "de83892b1d5596fc600060f48cb3e4aa"}
	{
	"name": "dynamo",
	"ts": 1757839471143148.5,
	"args": {
	"compile_id": "0/0",
	"guard_latency_us": 93,
	"frame_key": "1",
	"co_name": "forward",
	"co_filename": "/usr/local/lib/python3.12/dist-packages/litgpt/model.py",
	"co_firstlineno": 74,
	"cache_size": 0,
	"accumulated_cache_size": 0,
	"guard_count": 483,
	"shape_env_guard_count": 0,
	"graph_op_count": 23,
	"graph_node_count": 50,
	"graph_input_count": 24,
	"fail_type": null,
	"fail_reason": null,
	"fail_user_frame_filename": null,
	"fail_user_frame_lineno": null,
	"non_compliant_ops": [],
	"compliant_custom_ops": [],
	"restart_reasons": [],
	"dynamo_time_before_restart_s": 0.0,
	"has_guarded_code": true,
	"dynamo_config": "{\"_autograd_backward_strict_mode_conditional_banned_ops\": [\"stride\", \"storage_offset\", \"is_contiguous\"], \"_unsafe_skip_fsdp_module_guards\": false, \"accumulated_recompile_limit\": 256, \"allow_complex_guards_as_runtime_asserts\": false, \"allow_empty_graphs\": false, \"allow_ignore_mark_dynamic\": false, \"allow_rnn\": false, \"allow_unspec_int_on_nn_module\": false, \"allowed_functions_module_string_ignorelist\": [\"torch._decomp\", \"torch._prims\", \"torch._refs\", \"torch.distributions\", \"torch.testing\"], \"assume_static_by_default\": true, \"automatic_dynamic_local_pgo\": true, \"automatic_dynamic_remote_pgo\": null, \"automatic_dynamic_shapes\": true, \"automatic_dynamic_shapes_mark_as\": \"dynamic\", \"caching_precompile\": false, \"capture_autograd_function\": true, \"capture_dynamic_output_shape_ops\": false, \"capture_func_transforms\": true, \"capture_scalar_outputs\": false, \"capture_sparse_compute\": true, \"compiled_autograd\": false, \"compiled_autograd_kwargs_override\": {}, \"cprofile\": false, \"cudagraph_backend_keep_input_mutation\": false, \"cudagraph_backend_support_input_mutation\": false, \"dead_code_elimination\": true, \"disable\": false, \"do_not_emit_runtime_asserts\": false, \"dont_skip_tracing\": false, \"dynamic_shapes\": true, \"enable_compiler_collectives\": false, \"enable_cpp_framelocals_guard_eval\": true, \"enable_cpp_guard_manager\": true, \"enable_cpp_symbolic_shape_guards\": true, \"enable_faithful_generator_behavior\": true, \"enable_trace_contextlib\": true, \"enable_trace_unittest\": false, \"error_on_nested_fx_trace\": true, \"error_on_nested_jit_trace\": false, \"error_on_recompile\": false, \"fail_on_recompile_limit_hit\": false, \"fake_tensor_cache_crosscheck_enabled\": false, \"fake_tensor_cache_enabled\": true, \"fake_tensor_disable_inference_mode\": true, \"force_nn_module_property_static_shapes\": true, \"force_parameter_static_shapes\": true, \"force_unspec_int_unbacked_size_like_on_torchrec_kjt\": false, \"graph_break_on_nn_param_ctor\": true, \"graph_deduplication_lint\": false, \"guard_nn_modules\": true, \"guard_nn_modules_using_dict_tags\": true, \"inline_inbuilt_nn_modules\": true, \"install_free_tensors\": false, \"issue_3_13_0_warning\": true, \"max_saved_pointers_for_recursive_dict_tags_check\": 256, \"minimum_call_count\": 1, \"numpy_default_complex\": \"complex128\", \"numpy_default_float\": \"float64\", \"numpy_default_int\": \"int64\", \"only_allow_pt2_compliant_ops\": false, \"optimize_ddp\": true, \"optimize_ddp_lazy_compile\": false, \"prefer_deferred_runtime_asserts_over_guards\": false, \"prepare_freezing\": false, \"pt2_compile_id_prefix\": null, \"raise_on_ctx_manager_usage\": true, \"raise_on_unsafe_aot_autograd\": false, \"recompile_limit\": 8, \"record_compile_time_instruction_count\": false, \"record_runtime_overhead\": true, \"replay_record_enabled\": false, \"report_guard_failures\": true, \"rewrite_assert_with_torch_assert\": true, \"run_gc_after_compile\": true, \"skip_code_recursive_on_recompile_limit_hit\": true, \"skip_fsdp_guards\": true, \"skip_fsdp_hooks\": true, \"skip_guards_on_constant_func_defaults\": true, \"skip_nnmodule_hook_guards\": true, \"skip_no_tensor_aliasing_guards_on_parameters\": true, \"skip_tensor_guards_with_matching_dict_tags\": true, \"skip_torchrec\": true, \"skipfiles_inline_module_allowlist\": {}, \"specialize_float\": false, \"specialize_int\": false, \"suppress_errors\": false, \"trace_numpy\": true, \"track_nodes_for_deduplication\": false, \"use_graph_deduplication\": false, \"use_lamba_guard_for_object_aliasing\": true, \"use_lazy_graph_module\": true, \"use_numpy_random_stream\": false, \"use_recursive_dict_tags_for_guards\": true, \"verify_correctness\": false, \"wrap_top_frame\": false}"
	},
	"ph": "E",
	"cat": "dynamo_timed",
	"tid": 0,
	"pid": 0
	}
